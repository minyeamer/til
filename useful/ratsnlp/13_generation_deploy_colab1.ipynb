{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generation-deploy-colab1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oaGGhdmYKqt"
      },
      "source": [
        "# 패키지 설치\n",
        "pip 명령어로 의존성 있는 패키지를 설치합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8TJkXkpDnSq"
      },
      "source": [
        "!pip install ratsnlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3mThtbxyNyO"
      },
      "source": [
        "# 모델 로딩\n",
        "프리트레인한 GPT2 모델과 토크나이저를 읽어 들입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFV031RZFRgD"
      },
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "model = GPT2LMHeadModel.from_pretrained(\n",
        "    \"skt/kogpt2-base-v2\",\n",
        ")\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3amlsjpFd9i"
      },
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
        "    \"skt/kogpt2-base-v2\",\n",
        "    eos_token=\"</s>\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_b-3ZCPNQ5p"
      },
      "source": [
        "## 프롬프트 준비\n",
        "\n",
        "언어모델에 넣을 프롬프트를 준비합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhGrFki_M6CT"
      },
      "source": [
        "input_ids = tokenizer.encode(\"안녕하세요\", return_tensors=\"pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0yIwXhDNl7i"
      },
      "source": [
        "## Greedy Search\n",
        "\n",
        "\n",
        "다음 단어 확률 분포에서 최대 확률을 내는 단어들을 리턴합니다. \n",
        "여러 번 수행하더라도 생성 결과가 바뀌지 않습니다 (`do_sample=False`).\n",
        "`max_length`는 생성 최대 길이이며 이보다 길거나, 짧더라도 EOD 토큰 등 스페셜 토큰이 나타나면 생성을 중단합니다. `min_length`는 생성 최소 길이이며 이보다 짧은 구간에서 EOD 등 스페셜 토큰이 등장해 생성이 중단될 경우 해당 토큰이 나올 확률을 0으로 수정하여 문장 생성이 종료되지 않도록 강제합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J94NJQRHM9sA"
      },
      "source": [
        "import torch\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=False,\n",
        "        min_length=10,\n",
        "        max_length=50,\n",
        "    )\n",
        "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlpAZmbVQ0FN"
      },
      "source": [
        "## Beam Search \n",
        "\n",
        "Beam Search는 다음 단어 확률 분포에서 `num_beams`만큼의 경우의 수를 남겨가면서 문장을 생성합니다. Beam search는 Greedy search보다 계산량이 많지만 좀 더 확률값이 높은 문장을 생성할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZMWKWTyRLA4"
      },
      "source": [
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=False,\n",
        "        min_length=10,\n",
        "        max_length=50,\n",
        "        num_beams=3,\n",
        "    )\n",
        "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SJsuYFKREfp"
      },
      "source": [
        "`num_beams=1`로 설정하면 정확히 Greedy search와 동일하게 작동합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAioTRj3SuhA"
      },
      "source": [
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=False,\n",
        "        min_length=10,\n",
        "        max_length=50,\n",
        "        num_beams=1,\n",
        "    )\n",
        "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoVeZTxtQDpU"
      },
      "source": [
        "## 반복 줄이기\n",
        "\n",
        "### 반복되는 n-gram 사이즈를 지정하기\n",
        "\n",
        "위의 예시를 보면 `\"그럼, 그건 뭐예요?\"`이 반복됩니다. 이를 아래와 같이 지정해 반복을 방지합니다. 3개 이상의 토큰이 반복될 경우 해당 3-gram 등장 확률을 0으로 만들어 생성 결과에서 배제합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7-ixOKZQYmb"
      },
      "source": [
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=False,\n",
        "        min_length=10,\n",
        "        max_length=50,\n",
        "        no_repeat_ngram_size=3,\n",
        "    )\n",
        "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KMSLJm6QnqW"
      },
      "source": [
        "### repetition penalty\n",
        "\n",
        "repetition penalty로 반복을 통제할 수도 있습니다. 다음과 같이 실행하면 되며 그 범위는 1 이상의 값을 가져야 합니다. 1이라면 아무런 패널티를 적용하지 않는게 됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-E8zQaKaI0Y"
      },
      "source": [
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=False,\n",
        "        min_length=10,\n",
        "        max_length=50,\n",
        "        repetition_penalty=1.0,\n",
        "    )\n",
        "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA4sIZ3l2UGD"
      },
      "source": [
        "`repetition_penalty` 값이 클 수록 패널티가 세게 적용됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYYEgzc_QnLm"
      },
      "source": [
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=False,\n",
        "        min_length=10,\n",
        "        max_length=50,\n",
        "        repetition_penalty=1.1,\n",
        "    )\n",
        "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIAHKtfIZ93g"
      },
      "source": [
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=False,\n",
        "        min_length=10,\n",
        "        max_length=50,\n",
        "        repetition_penalty=1.2,\n",
        "    )\n",
        "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qe5hlCgaRAL"
      },
      "source": [
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=False,\n",
        "        min_length=10,\n",
        "        max_length=50,\n",
        "        repetition_penalty=1.5,\n",
        "    )\n",
        "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG-THCtUd8BP"
      },
      "source": [
        "## top-k sampling\n",
        "\n",
        "지금까지는 생성을 반복하더라도 그 결과가 동일한 샘플링 방식을 살펴봤습니다. top-k sampling은 다음 단어를 뽑을 때 확률값 기준 가장 큰 k개 가운데 하나를 선택하는 기법입니다. 확률값이 큰 단어가 다음 단어로 뽑힐 가능성이 높아지지만, k개 안에 있는 단어라면 확률값이 낮더라도 다음 단어로 추출될 수 있습니다. 따라서 top-k sampling은 매 시행 때마다 생성 결과가 달라집니다. k는 1 이상의 값을 지녀야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZykqqkLeoGa"
      },
      "source": [
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=True,\n",
        "        min_length=10,\n",
        "        max_length=50,\n",
        "        top_k=50,\n",
        "    )\n",
        "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65_mnOwW219w"
      },
      "source": [
        "k=1일 경우 Greedy search와 동일합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RGBeGUMeoqx"
      },
      "source": [
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=True,\n",
        "        min_length=10,\n",
        "        max_length=50,\n",
        "        top_k=1,\n",
        "    )\n",
        "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnI1PwaSgAN3"
      },
      "source": [
        "## top-k sampling + temperature scaling\n",
        "\n",
        "top-k sampling은 temperature scaling과 동시에 적용할 수 있습니다. 그 값에 따라 다음과 같은 효과가 납니다.\n",
        "\n",
        "(1) t가 0에 가까워질 수록 토큰 분포가 sharp해진다 > 1등 토큰이 뽑힐 확률이 그만큼 높아진다 > do_sample=True이지만 사실상 greedy decoding이 된다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OelobEFugAeV"
      },
      "source": [
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=True,\n",
        "        min_length=10,\n",
        "        max_length=50,\n",
        "        top_k=50,\n",
        "        temperature=0.01,\n",
        "    )\n",
        "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u2O8Vfc3IET"
      },
      "source": [
        "(2) t=1이라면 모델 출력 분포를 그대로 사용한다 > 하지만 샘플링 방식을 사용하기 때문에 생성할 때마다 다른 문장이 나온다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-wV6jAdiF_D"
      },
      "source": [
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=True,\n",
        "        min_length=10,\n",
        "        max_length=50,\n",
        "        top_k=50,\n",
        "        temperature=1.0,\n",
        "    )\n",
        "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qJla1_y3N1y"
      },
      "source": [
        "(3) t를 키울수록 토큰 분포가 uniform해진다 > 사실상 uniform sampling이 된다, 생성 품질이 악화할 가능성이 높아진다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnrgKCxuiMrN"
      },
      "source": [
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=True,\n",
        "        min_length=10,\n",
        "        max_length=50,\n",
        "        top_k=50,\n",
        "        temperature=100000000.0,\n",
        "    )\n",
        "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50bBi6ZRihKw"
      },
      "source": [
        "## top-p sampling\n",
        "\n",
        "top-p sampling은 다음 단어를 뽑을 때 누적 확률값이 p 이하인 단어들 가운데 하나를 선택하는 기법입니다. 확률값이 큰 단어가 다음 단어로 뽑힐 가능성이 높아지지만, 누적 확률값 p 이하에 있는 단어라면 확률값이 낮더라도 다음 단어로 추출될 수 있습니다. 따라서 top-p sampling은 매 시행 때마다 생성 결과가 달라집니다. p는 확률이기 때문에 0~1 사이의 값을 지녀야 합니다. p가 1이라면 어휘 집합에 있는 모든 단어를 대상으로 샘플링하기 때문에 top-p sampling 효과가 사라집니다. p가 1보다 약간 작다면 확률값이 낮은 일부 단어들을 다음 단어 후보에서 제거해 생성 품질을 높입니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjNy7gaXkDGn"
      },
      "source": [
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=True,\n",
        "        min_length=10,\n",
        "        max_length=50,\n",
        "        top_p=0.92,\n",
        "    )\n",
        "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIws234s4JhR"
      },
      "source": [
        "p가 0에 가까울 경우 Greedy search와 비슷해 집니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCAgYU3RkZ80"
      },
      "source": [
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=True,\n",
        "        min_length=10,\n",
        "        max_length=50,\n",
        "        top_p=0.01,\n",
        "    )\n",
        "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRR-vy6klf8y"
      },
      "source": [
        "## 통합 적용\n",
        "\n",
        "저희가 실습에 사용하고 있는 허깅페이스(huggingface) 라이브러리의 구현상 적용 순서는 다음과 같습니다.\n",
        "\n",
        "- _get_logits_processor\n",
        "  - RepetitionPenalty\n",
        "  - NoRepeatNGramLogits\n",
        "  - MinLengthLogits\n",
        "- _get_logits_warper\n",
        "  - TemperatureLogits\n",
        "  - TopKLogits\n",
        "  - TopPLogits\n",
        "\n",
        "유효한 설정들을 종합 적용해 문장을 생성하는 코드는 다음과 같습니다. 샘플링(top-k, top-p)을 적용하기 때문에 시행 때마다 다른 문장이 생성됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLo9mtlUmcAK"
      },
      "source": [
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=True,\n",
        "        min_length=10,\n",
        "        max_length=50,\n",
        "        repetition_penalty=1.5,\n",
        "        no_repeat_ngram_size=3,\n",
        "        temperature=0.9,\n",
        "        top_k=50,\n",
        "        top_p=0.92,\n",
        "    )\n",
        "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}