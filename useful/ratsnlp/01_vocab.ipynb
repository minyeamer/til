{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oaGGhdmYKqt"
      },
      "source": [
        "# 패키지 설치\n",
        "pip 명령어로 의존성 있는 패키지를 설치합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8TJkXkpDnSq"
      },
      "outputs": [],
      "source": [
        "!pip install ratsnlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvPQbN7vs50o"
      },
      "source": [
        "# 구글 드라이브 연동하기\n",
        "\n",
        "이번 튜토리얼에서 구축할 어휘 집합을 저장해 둘 구글 드라이브를 연결합니다. 자신의 구글 계정에 적용됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpjMKSVWs6B2",
        "outputId": "11bce06d-f59f-4fba-9c11-290fcbe1560a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFt_JLYoTKA6"
      },
      "source": [
        "## 말뭉치 다운로드 및 전처리\n",
        "\n",
        "오픈소스 파이썬 패키지 코포라(Korpora)를 활용해 BPE 수행 대상 말뭉치를 내려받고 전처리합니다. 실습용 말뭉치는 박은정 님이 공개하신 Naver Sentiment Movie Corpus(NSMC)입니다.\n",
        "\n",
        "다음을 수행해 데이터를 내려받아 `nsmc`라는 변수로 읽어들입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bThW-2OrTHZW",
        "outputId": "8cb971e9-abca-474e-c890-b2826aa2e245"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nsmc] download ratings_train.txt: 14.6MB [00:00, 81.1MB/s]                           \n",
            "[nsmc] download ratings_test.txt: 4.90MB [00:00, 36.1MB/s]                           \n"
          ]
        }
      ],
      "source": [
        "from Korpora import Korpora\n",
        "nsmc = Korpora.load(\"nsmc\", force_download=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0NCnzpRTdnJ"
      },
      "source": [
        "다음을 수행하면 NSMC에 포함된 영화 리뷰(순수 텍스트)들을 지정된 경로에 저장해 둡니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FSVDbnFUTiXx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def write_lines(path, lines):\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        for line in lines:\n",
        "            f.write(f'{line}\\n')\n",
        "\n",
        "write_lines(\"/content/train.txt\", nsmc.train.get_all_texts())\n",
        "write_lines(\"/content/test.txt\", nsmc.test.get_all_texts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWboOauTTyXp"
      },
      "source": [
        "`train.txt`의 앞부분은 다음과 같이 생겼습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tup5LreLT4vE",
        "outputId": "58750817-7ec8-49a3-b0ea-09baa06109fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "아 더빙.. 진짜 짜증나네요 목소리\n",
            "흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
            "너무재밓었다그래서보는것을추천한다\n",
            "교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n",
            "사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\n",
            "막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.\n",
            "원작의 긴장감을 제대로 살려내지못했다.\n",
            "별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단 낫겟다 납치.감금만반복반복..이드라마는 가족도없다 연기못하는사람만모엿네\n",
            "액션이 없는데도 재미 있는 몇안되는 영화\n",
            "왜케 평점이 낮은건데? 꽤 볼만한데.. 헐리우드식 화려함에만 너무 길들여져 있나?\n"
          ]
        }
      ],
      "source": [
        "!head train.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMKHuGgyT8aw"
      },
      "source": [
        "`test.txt`의 앞부분은 다음과 같이 생겼습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI5EpyyaT_fl",
        "outputId": "68a558e0-5e72-4195-aa27-e72a97e20544"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "굳 ㅋ\n",
            "GDNTOPCLASSINTHECLUB\n",
            "뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아\n",
            "지루하지는 않은데 완전 막장임... 돈주고 보기에는....\n",
            "3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??\n",
            "음악이 주가 된, 최고의 음악영화\n",
            "진정한 쓰레기\n",
            "마치 미국애니에서 튀어나온듯한 창의력없는 로봇디자인부터가,고개를 젖게한다\n",
            "갈수록 개판되가는 중국영화 유치하고 내용없음 폼잡다 끝남 말도안되는 무기에 유치한cg남무 아 그립다 동사서독같은 영화가 이건 3류아류작이다\n",
            "이별의 아픔뒤에 찾아오는 새로운 인연의 기쁨 But, 모든 사람이 그렇지는 않네..\n"
          ]
        }
      ],
      "source": [
        "!head test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y2JYqRGUFll"
      },
      "source": [
        "## GPT 토크나이저 구축\n",
        "\n",
        "GPT 계열 모델이 사용하는 토크나이저는 Byte-level Byte Pair Encoding(BBPE)입니다. 우선 어휘집합 구축 결과를 저장해둘 디렉토리를 자신의 구글 드라이브 계정 내 `내 드라이브/nlpbook/bbpe`로 만들어 둡니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HMsp3GRVuGg7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"/gdrive/My Drive/nlpbook/bbpe\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2jwK4iwvB1l"
      },
      "source": [
        "다음을 수행하면 `nsmc` 데이터를 가지고 BBPE 어휘집합을 구축합니다. BBPE 어휘집합 구축에 시간이 걸리니 잠시 기다려주세요. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Rk2Ga65USFb",
        "outputId": "c089a554-a6d9-457f-a8ad-1083322a88e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/gdrive/My Drive/nlpbook/bbpe/vocab.json',\n",
              " '/gdrive/My Drive/nlpbook/bbpe/merges.txt']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "bytebpe_tokenizer = ByteLevelBPETokenizer()\n",
        "bytebpe_tokenizer.train(\n",
        "    files=[\"/content/train.txt\", \"/content/test.txt\"],\n",
        "    vocab_size=10000,\n",
        "    special_tokens=[\"[PAD]\"]\n",
        ")\n",
        "bytebpe_tokenizer.save_model(\"/gdrive/My Drive/nlpbook/bbpe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLq1JRVJUb7U"
      },
      "source": [
        "위의 코드 수행이 끝나면 자신의 구글 드라이브 경로(`/gdrive/My Drive/nlpbook/bbpe`)에 `vocab.json`과 `merges.txt`가 생성됩니다. 전자는 바이트 레벨 BPE의 어휘 집합이며 후자는 바이그램 쌍의 병합 우선순위입니다. \n",
        "\n",
        "`vocab.json`은 다음과 같이 생겼습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZl8OKdsVIQg"
      },
      "outputs": [],
      "source": [
        "!cat /gdrive/My\\ Drive/nlpbook/bbpe/vocab.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etZsJ4SrVXnS"
      },
      "source": [
        "`merges.txt`의 앞부분은 다음과 같이 생겼습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzkJ31vQVbjl",
        "outputId": "09189c84-e76f-4eb6-a208-3c4291f369ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#version: 0.2 - Trained by `huggingface/tokenizers`\n",
            "Ġ ì\n",
            "Ġ ë\n",
            "ì Ŀ\n",
            "ë ĭ\n",
            "í ķ\n",
            "ê °\n",
            ". .\n",
            "ìĿ ´\n",
            "ëĭ ¤\n"
          ]
        }
      ],
      "source": [
        "!head /gdrive/My\\ Drive/nlpbook/bbpe/merges.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wR-3V6zIVsPp"
      },
      "source": [
        "## BERT 토크나이저 구축\n",
        "\n",
        "BERT는 워드피스(wordpiece) 토크나이저를 사용합니다. 우선 어휘집합 구축 결과를 저장해둘 디렉토리를 자신의 구글 드라이브 계정 내 `내 드라이브/nlpbook/bbpe`로 만들어 둡니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aObMCetww3YU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"/gdrive/My Drive/nlpbook/wordpiece\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzmRa-NPw3l_"
      },
      "source": [
        "다음을 수행하면 BERT 모델이 사용하는 워드피스 어휘집합을 구축할 수 있습니다. 워드피스 어휘집합 구축에 시간이 걸리니 잠시만 기다려주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdvUPuJoV3w3",
        "outputId": "2f63deae-98b0-412b-f553-75f59e1ccb10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/gdrive/My Drive/nlpbook/wordpiece/vocab.txt']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "wordpiece_tokenizer = BertWordPieceTokenizer(lowercase=False)\n",
        "wordpiece_tokenizer.train(\n",
        "    files=[\"/content/train.txt\", \"/content/test.txt\"],\n",
        "    vocab_size=10000,\n",
        ")\n",
        "wordpiece_tokenizer.save_model(\"/gdrive/My Drive/nlpbook/wordpiece\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0S2EbdkWIdq"
      },
      "source": [
        "위의 코드 수행이 끝나면 자신의 구글 드라이브 경로(`/gdrive/My Drive/nlpbook/wordpiece`)에 `vocab.txt`가 생성됩니다. `vocab.txt`의 앞부분은 다음과 같이 생겼습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOWkywHaWOL6",
        "outputId": "c0ec6f0d-116b-47aa-be9c-21971a256f01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PAD]\n",
            "[UNK]\n",
            "[CLS]\n",
            "[SEP]\n",
            "[MASK]\n",
            "!\n",
            "\"\n",
            "%\n",
            "&\n",
            "'\n"
          ]
        }
      ],
      "source": [
        "!head /gdrive/My\\ Drive/nlpbook/wordpiece/vocab.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6Bl9wOPi76e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "vocab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 64-bit ('mldl')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "86ae205601b6d906014fa7892090616f7e1469eb0aa86f06d2d1803a695f1eb6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
