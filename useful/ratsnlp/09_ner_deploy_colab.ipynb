{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ner-deploy-colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oaGGhdmYKqt"
      },
      "source": [
        "# 패키지 설치\n",
        "pip 명령어로 의존성 있는 패키지를 설치합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8TJkXkpDnSq"
      },
      "source": [
        "!pip install ratsnlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppGzJeg_x12T"
      },
      "source": [
        "# 구글 드라이브 연동하기\n",
        "모델 체크포인트 등을 저장해 둘 구글 드라이브를 연결합니다. 자신의 구글 계정에 적용됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgSyL_BsVTfl"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC5OwyKMx_l9"
      },
      "source": [
        "# 각종 설정\n",
        "모델 하이퍼파라메터(hyperparameter)와 저장 위치 등 설정 정보를 선언합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKybDwDqFIX5"
      },
      "source": [
        "from ratsnlp.nlpbook.ner import NERDeployArguments\n",
        "args = NERDeployArguments(\n",
        "    pretrained_model_name=\"beomi/kcbert-base\",\n",
        "    downstream_model_dir=\"/gdrive/My Drive/nlpbook/checkpoint-ner\",\n",
        "    max_seq_length=64,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3mThtbxyNyO"
      },
      "source": [
        "# 모델 로딩\n",
        "파인튜닝을 마친 모델과 토크나이저를 읽어 들입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFV031RZFRgD"
      },
      "source": [
        "import torch\n",
        "from transformers import BertConfig, BertForTokenClassification\n",
        "fine_tuned_model_ckpt = torch.load(\n",
        "    args.downstream_model_checkpoint_fpath,\n",
        "    map_location=torch.device(\"cpu\")\n",
        ")\n",
        "pretrained_model_config = BertConfig.from_pretrained(\n",
        "    args.pretrained_model_name,\n",
        "    num_labels=fine_tuned_model_ckpt['state_dict']['model.classifier.bias'].shape.numel(),\n",
        ")\n",
        "model = BertForTokenClassification(pretrained_model_config)\n",
        "model.load_state_dict({k.replace(\"model.\", \"\"): v for k, v in fine_tuned_model_ckpt['state_dict'].items()})\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3amlsjpFd9i"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    args.pretrained_model_name,\n",
        "    do_lower_case=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJGScC_1g6Ni"
      },
      "source": [
        "# 레이블 맵 작성\n",
        "\n",
        "범주 인덱스를 범주명과 매칭하는 사전을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4g9hNcrhFru"
      },
      "source": [
        "labels = [label.strip() for label in open(args.downstream_model_labelmap_fpath, \"r\").readlines()]\n",
        "id_to_label = {}\n",
        "for idx, label in enumerate(labels):\n",
        "  if \"PER\" in label:\n",
        "    label = \"인명\"\n",
        "  elif \"LOC\" in label:\n",
        "    label = \"지명\"\n",
        "  elif \"ORG\" in label:\n",
        "    label = \"기관명\"\n",
        "  elif \"DAT\" in label:\n",
        "    label = \"날짜\"\n",
        "  elif \"TIM\" in label:\n",
        "    label = \"시간\"\n",
        "  elif \"DUR\" in label:\n",
        "    label = \"기간\"\n",
        "  elif \"MNY\" in label:\n",
        "    label = \"통화\"\n",
        "  elif \"PNT\" in label:\n",
        "    label = \"비율\"\n",
        "  elif \"NOH\" in label:\n",
        "    label = \"기타 수량표현\"\n",
        "  elif \"POH\" in label:\n",
        "    label = \"기타\"\n",
        "  else:\n",
        "    label = label\n",
        "  id_to_label[idx] = label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWVsdmThyV_p"
      },
      "source": [
        "# 인퍼런스 함수 선언\n",
        "인퍼런스 함수를 선언합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnzR9NMtFiAz"
      },
      "source": [
        "def inference_fn(sentence):\n",
        "    inputs = tokenizer(\n",
        "        [sentence],\n",
        "        max_length=args.max_seq_length,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**{k: torch.tensor(v) for k, v in inputs.items()})\n",
        "        probs = outputs.logits[0].softmax(dim=1)\n",
        "        top_probs, preds = torch.topk(probs, dim=1, k=1)\n",
        "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "        predicted_tags = [id_to_label[pred.item()] for pred in preds]\n",
        "        result = []\n",
        "        for token, predicted_tag, top_prob in zip(tokens, predicted_tags, top_probs):\n",
        "            if token not in [tokenizer.pad_token, tokenizer.cls_token, tokenizer.sep_token]:\n",
        "                token_result = {\n",
        "                    \"token\": token,\n",
        "                    \"predicted_tag\": predicted_tag,\n",
        "                    \"top_prob\": str(round(top_prob[0].item(), 4)),\n",
        "                }\n",
        "                result.append(token_result)\n",
        "    return {\n",
        "        \"sentence\": sentence,\n",
        "        \"result\": result,\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 웹서비스 만들기 준비\n",
        "\n",
        "`ngrok`은 코랩 로컬에서 실행 중인 웹서비스를 안전하게 외부에서 접근 가능하도록 해주는 도구입니다. `ngrok`을 실행하려면 [회원가입](https://dashboard.ngrok.com/signup) 후 [로그인](https://dashboard.ngrok.com/login)을 한 뒤 [이곳](https://dashboard.ngrok.com/get-started/your-authtoken)에 접속해 인증 토큰(authtoken)을 확인해야 합니다. 예를 들어 확인된 `authtoken`이 `test111`이라면 다음과 같이 실행합니다.\n",
        "\n",
        "```bash\n",
        "!mkdir /root/.ngrok2 && echo \"authtoken: test111\" > /root/.ngrok2/ngrok.yml\n",
        "```"
      ],
      "metadata": {
        "id": "Xt7Z7G0dB7yY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /root/.ngrok2 && echo \"authtoken: {이곳에 확인된 인증 토큰을 입력하세요}\" > /root/.ngrok2/ngrok.yml"
      ],
      "metadata": {
        "id": "6KshHb4P_0wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPP6ZAaSybge"
      },
      "source": [
        "# 웹서비스 개시\n",
        "아래처럼 실행해 인퍼런스 함수를 웹서비스로 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_up1ARoHFwLN"
      },
      "source": [
        "from ratsnlp.nlpbook.ner import get_web_service_app\n",
        "app = get_web_service_app(inference_fn)\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}