# Deep Learning
1. [Deep Learning](#1-deep-learning)
2. [Perceptron](#2-perceptron)
3. [Activation Function](#3-activation-function)
4. [Neural Network Optimization](#4-neural-network-optimization)

---

## 1. Deep Learning
- ì•„ì£¼ ë§ì€ ë°ì´í„°ë¥¼ ì´ìš©í•œ í•™ìŠµ
- ë¨¸ì‹ ëŸ¬ë‹ì˜ Feature Engineering ê³¼ì •ì„ ì»´í“¨í„°ê°€ ì§„í–‰ (Feature Extraction)
- ë°ì´í„°ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ì‹œê°„ì´ ëŠ˜ì–´ë‚˜ì§€ë§Œ í•™ìŠµì˜ ê¹Šì´ë„ ì¦ê°€
- Data Augmentation: ë¶€ì¡±í•œ ë°ì´í„°ë¥¼ ì¼ë¶€ ë³€í˜•í•˜ì—¬ ë°ì´í„°ì˜ ìˆ˜ ì¦ê°€

---

## 2. Perceptron
- Thresholdë¥¼ ë„˜ê¸°ë©´ ë‹¤ìŒ ë‰´ëŸ°ì—ê²Œ ì „ê¸°ì‹ í˜¸ë¥¼ ë³´ë‚´ëŠ” ë‰´ëŸ°ì„ ë³¸ë”´ ì¸ê³µì‹ ê²½ë§
- ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ë©´ ë‚´ë¶€ì ìœ¼ë¡œ ì—°ì‚°ì„ ì§„í–‰í•´ í•˜ë‚˜ì˜ ê°’ìœ¼ë¡œ ë‹¤ìŒ í¼ì…‰íŠ¸ë¡ ì—ê²Œ ì „ë‹¬
- ê¹Šì´ (ì¸µì˜ ê°œìˆ˜), ë„ˆë¹„ (í•˜ë‚˜ì˜ ì¸µì— ìˆëŠ” í¼ì…‰íŠ¸ë¡  ìˆ˜)
- í¼ì…‰íŠ¸ë¡ ì„ ì—°ê²°í•˜ëŠ” ì„  í•˜ë‚˜í•˜ë‚˜ê°€ `Î¸`ë¥¼ ì˜ë¯¸
- ì œì¼ ì¢‹ì€ `Î¸` ê°’ì˜ ì¡°í•©ì„ ì°¾ëŠ” ê²ƒì´ ëª©ì 

### Single-Layer Perceptron
- ë‰´ëŸ°ì„ ë³¸ë”° ë§Œë“  ì•Œê³ ë¦¬ì¦˜ í•˜ë‚˜ì˜ ë‹¨ìœ„
- ë°ì´í„°ì™€ `Î¸`ì˜ ì„ í˜• ê²°í•©ì„ ê³„ì‚°í•˜ê³  ê²°ê³¼ì— Activation Function ì ìš©
- Activaiton Functionì€ ì„ í˜• ê²°ê³¼ë¥¼ êº¾ëŠ” ë¹„ì„ í˜• í•¨ìˆ˜   
  (ë‹¨ìˆœ ì„ í˜• íšŒê·€ë¡œëŠ” ì•„ë¬´ë¦¬ ë§ì€ Layerë¥¼ ìŒ“ì•„ë„ ê°™ì€ ì„ í˜• íšŒê·€ ê²°ê³¼ê°€ ë‚˜ì˜´)
- XOR ë¬¸ì œì˜ ê²½ìš° Linearly Inseparableí•˜ì—¬ í•˜ë‚˜ì˜ í¼ì…‰íŠ¸ë¡ ìœ¼ë¡œ í’€ ìˆ˜ ì—†ìŒ

### Multi-Layer Perceptron
- ë³µìˆ˜ì˜ í¼ì…‰íŠ¸ë¡ ì„ ì—°ê²°í•œ êµ¬ì¡°
- ë‹¨ì¸µ í¼ì…‰íŠ¸ë¡ ì—ì„œ í’€ ìˆ˜ ì—†ì—ˆë˜ XOR ë¬¸ì œë¥¼ and ì—°ì‚° ë‰´ëŸ°ê³¼ or ì—°ì‚° ë‰´ëŸ°ì˜ ì¡°í•©ìœ¼ë¡œ í•´ê²°

### (Artificial) Neural Network
- í¼ì…‰íŠ¸ë¡ ì„ ëª¨ì€ ë ˆì´ì–´ë¥¼ ê¹Šì´ ë°©í–¥ìœ¼ë¡œ ìŒ“ì•„ë‚˜ê°€ë©´ì„œ ë§Œë“¤ì–´ë‚¸ ì¸ê³µ ì‹ ê²½ë§ ëª¨ë¸
- Input Layer: ì™¸ë¶€ë¡œë¶€í„° ë°ì´í„°ë¥¼ ì…ë ¥ ë°›ëŠ” ì‹ ê²½ë§ ì…êµ¬
- Hidden Layer: Input Layerì™€ Output Layer ì‚¬ì´ì˜ ëª¨ë“  ë ˆì´ì–´ (Learnable Kernel)
- Output Layer: ëª¨ë¸ì˜ ìµœì¢… ì—°ì‚° ê²°ê³¼ë¥¼ ë‚´ë³´ë‚´ëŠ” ì‹ ê²½ë§ ì¶œêµ¬
- ê²°ê³¼ê°’ì„ ê·¸ëŒ€ë¡œ ë°›ìœ¼ë©´ Regression
- ê²°ê³¼ê°’ì— Sigmoidë¥¼ ê±°ì¹˜ë©´ Binary Classification
- ê²°ê³¼ê°’ì— Softmaxë¥¼ ê±°ì¹˜ë©´ K-Class Classificiation

### Back Propagation Algorithm
- ì˜¤ì°¨ ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ (ì‹ ê²½ë§ì˜ íš¨ìœ¨ì ì¸ í•™ìŠµ)
- í•™ìŠµëœ ì¶œë ¥ ê°’ê³¼ ì‹¤ì œ ê°’ê³¼ì˜ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•˜ì—¬ Feedforward ë°˜ëŒ€ì¸ ì—­ë°©í–¥ìœ¼ë¡œ ì „íŒŒí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜
- Forward ë°©í–¥ìœ¼ë¡œ í•œë²ˆ ì—°ì‚°ì„ í•œ ë‹¤ìŒ ê·¸ ê²°ê³¼ê°’(Cost)ë¥¼ ì—­ë°©í–¥ìœ¼ë¡œ ì „ë‹¬í•´ê°€ë©´ì„œ Parameterë¥¼ Update
- ëª¨ë¸ì´ í‹€ë¦° ì •ë„ë¥¼ ì—­ë°©í–¥ìœ¼ë¡œ ì „ë‹¬í•˜ë©° ë¯¸ë¶„í•˜ê³  ê³±í•˜ê³  ë”í•˜ëŠ” ê²ƒì„ ë°˜ë³µí•˜ì—¬ `Î¸`ë¥¼ ê°±ì‹ 
- Vanishing Gradient: ë ˆì´ì–´ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ ì•ì„  ì˜¤ì°¨ ê°’ì´ ì—­ë°©í–¥ìœ¼ë¡œ ë’¤ê¹Œì§€ ì „ë‹¬ì´ ë˜ì§€ ì•ŠëŠ” ë¬¸ì œ
- Activation Function(ReLu Function)ì„ ì ìš©í•˜ì—¬ ë¬¸ì œ í•´ê²°

---

## 3. Activation Function
- ì„ í˜• ê²°í•© ê²°ê³¼ì— ëŒ€í•œ ì¶œë ¥ ê°’ì„ ë‹¤ìŒ ë ˆì´ì–´ë¡œ ì „ë‹¬í•˜ëŠ” ë¹„ì„ í˜• í•¨ìˆ˜
- ë§ˆì§€ë§‰ ê²°ê³¼ì— ëŒ€í•œ MSEë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ Gradient Descentë¥¼ ì§„í–‰í•˜ëŠ”ë°,   
  ì´ì „ ì¸µì˜ `Î¸`ì— ëŒ€í•´ì„œëŠ” ë¯¸ë¶„ì„ í†µí•´ í•œë‹¨ê³„ ì „ì˜ ìƒíƒœë¡œ ë˜ëŒë ¤ ìˆ˜í–‰ (Back Propagation Algorithm)

### Step Function
- 0ë³´ë‹¤ í¬ë©´ 1, 0ë³´ë‹¤ ì‘ìœ¼ë©´ 0ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
- ë¯¸ë¶„ ì‹œ 0ì´ ë˜ì–´ ê²°ê³¼ë¥¼ ì´ì „ ìƒíƒœë¡œ ë˜ëŒë¦´ ìˆ˜ ì—†ëŠ” ë¬¸ì œë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ

### Sigmoid Function
- Step Functionê³¼ ìœ ì‚¬í•œ í˜•íƒœì˜ ì§€ìˆ˜ í•¨ìˆ˜
- `tanh`: Sigmoid Functionì€ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” y ê°’ì´ ì–‘ìˆ˜ ì˜ì—­ì— í•œì •ë˜ê¸° ë•Œë¬¸ì—   
  ê·¸ë˜í”„ë¥¼ ë‚´ë ¤ì„œ -1ì—ì„œ 1 ì‚¬ì´ ë²”ìœ„ë¡œ ë³€í™˜
- ê°€ì¥ í° ê¸°ìš¸ê¸° ê°’ì´ 0.25ì— ë¶ˆê³¼í•˜ê¸° ë•Œë¬¸ì— ê²°ê³¼ ê°’ì„ 5ë‹¨ê³„ë§Œ ì´ì „ìœ¼ë¡œ ë˜ëŒë ¤ë„   
  0.001ì— ê·¼ì ‘í•œ ì˜ë¯¸ì—†ëŠ” ê°’ì´ ë¨

### ReLu Function
- 0ë³´ë‹¤ í¬ë©´ x ê°’, 0ë³´ë‹¤ ì‘ìœ¼ë©´ 0ì„ ë°˜í™˜í•˜ëŠ” êº¾ì¸ í˜•íƒœì˜ í•¨ìˆ˜
- `Leaky ReLU`: ì¶œë ¥ì´ 0ì¼ ê²½ìš° ì–´ë–¤ ê²ƒì„ ê³±í•´ë„ 0ì´ ë˜ì–´ë²„ë¦¬ëŠ” Bad Neuronì´ ë°œìƒí•˜ê¸° ë–„ë¬¸ì—   
  0ë³´ë‹¤ ì‘ì€ ê°’ì— ëŒ€í•´ ë¯¸ì„¸í•œ ìŒìˆ˜ ê²°ê³¼ ê°’(y=âºx)ì„ ë°˜í™˜í•˜ë„ë¡ í•œ í•¨ìˆ˜
- `PReLU`: ì»´í“¨í„°ê°€ ì—ëŸ¬ë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ `âº` ê°’ì„ ì¡°ì •í•˜ëŠ” í•¨ìˆ˜ (Parametric ReLU)

---

## 4. Neural Network Optimization
- Gradient Descentë¥¼ í™œìš©í•œ ì‹ ê²½ë§ í•™ìŠµ
- (1)ëª¨ë“  Parameter `Î¸`ë¥¼ ì´ˆê¸°í™”, (2)Cost Function ìƒì˜ ê°€ì¥ ë‚®ì€ ì§€ì ìœ¼ë¡œ ì´ë™, (3)`Î¸`ë¥¼ Update

### Weight Initialization
- Gradient Descentë¥¼ ì ìš©í•˜ê¸° ìœ„í•´ Parameter `Î¸`ë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ê²ƒ
- Xavier Initialization: Sigmoid ë˜ëŠ” tanh í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ë–„ ì ìš©, í‘œì¤€í¸ì°¨ê°€ sqrt(1/n)ì¸ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ë„ë¡ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
- He Initialization: ReLu í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ë•Œ ì ìš©, í‘œì¤€í¸ì°¨ê°€ sqrt(2/n)ì¸ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ë„ë¡ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”

### Weight Regularization
- Gradient Descent ê³„ì‚° ì‹œ Cost Functionì€ Tradining Dataì— ëŒ€í•´ ëª¨ë¸ì´ ë°œìƒì‹œí‚¤ëŠ” ì—ëŸ¬ ê°’ì˜ ì§€í‘œ
- Training Dataë§Œ ê³ ë ¤ëœ Cost Functionì„ ê¸°ì¤€ìœ¼ë¡œ Gradient Descentë¥¼ ì ìš©í•˜ë©´ Overfitting ìš°ë ¤
- ëª¨ë¸ì´ ë³µì¡í•´ì§ˆìˆ˜ë¡ ëª¨ë¸ ì†ì— ìˆ¨ì–´ìˆëŠ” `Î¸`ì˜ ê°œìˆ˜ê°€ ì¦ê°€í•˜ê³  ì ˆëŒ€ê°’ì´ ì»¤ì§
- `Î¸`ì— ëŒ€í•œ í•¨ìˆ˜(Regularization Term)ë¥¼ ê¸°ì¡´ì˜ Cost Functionì— ë”í•˜ì—¬ Trade-off ê´€ê³„ ì†ì—ì„œ ìµœì ê°’ì„ ì°¾ìŒ

### Regularization Term
- `J(Î¸) = MSE + Î»R(Î¸)`, Weight Decay
- L1 Regularization: ê°€ì¤‘ì¹˜ì˜ ì ˆëŒ€ê°’ì˜ í•©ì— ë¹„ë¡€í•˜ì—¬ ê°€ì¤‘ì¹˜ì— íŒ¨ë„í‹° (Lasso, ë§ˆë¦„ëª¨ê¼´),   
  ê´€ë ¨ì„±ì´ ì—†ê±°ë‚˜ ë§¤ìš° ë‚®ì€ íŠ¹ì„±ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì •í™•íˆ 0ìœ¼ë¡œ ìœ ë„í•˜ì—¬ ëª¨ë¸ì—ì„œ í•´ë‹¹ íŠ¹ì„± ë°°ì œ
- L2 Regularization: ê°€ì¤‘ì¹˜ì˜ ì œê³±ì˜ í•©ì— ë¹„ë¡€í•˜ì—¬ ê°€ì¤‘ì¹˜ì— íŒ¨ë„í‹° (Ridge, ì›ì˜ ë°©ì •ì‹),   
  í° ê°’ì„ ê°€ì§„ ê°€ì¤‘ì¹˜ë¥¼ ë”ìš± ì œì•½í•˜ëŠ” íš¨ê³¼
- Regularization Rate: ìŠ¤ì¹¼ë¼ ê°’ (Lambda, L1/L2 ë„í˜•ì˜ í¬ê¸°),   
  ì •ê·œí™” í•¨ìˆ˜ì˜ ìƒëŒ€ì  ì¤‘ìš”ë„ ì§€ì •, ì •ê·œí™”ìœ¨ì„ ë†’ì´ë©´ ê³¼ì í•©ì´ ê°ì†Œí•˜ì§€ë§Œ ëª¨ë¸ì˜ ì •í™•ì„±ì´ ë–¨ì–´ì§

### Advanced Gradient Descent Algorithms
- Stochastic Gradient Descent: í•˜ë‚˜ì˜ Training Data ë§ˆë‹¤ Costë¥¼ ê³„ì‚°í•˜ê³  Gradient Descent ì ìš©,   
  ë§¤ë²ˆ weightë¥¼ ê°±ì‹ í•˜ê¸° ë•Œë¬¸ì— ì‹ ê²½ë§ì˜ ì„±ëŠ¥ì´ ë¶ˆì•ˆì •, ìˆ˜ë ´ì¡°ê±´ì„ ì¡°ì •í•´ì•¼í•  í•„ìš”
- Mini-Batch Stochasitc Gradient Descent: Training Dataì—ì„œ ì¼ì •í•œ í¬ê¸°(Batch Size)ì˜ ë°ì´í„° ì„ íƒ,   
  ì„¤ê³„ìì˜ ì˜ë„ì— ë”°ë¼ ì†ë„ì™€ ì•ˆì •ì„±ì„ ë™ì‹œì— ê´€ë¦¬, GPU ê¸°ë°˜ì˜ íš¨ìœ¨ì ì¸ ë³‘ë ¬ ì—°ì‚° ê°€ëŠ¥
- Epoch: ì „ì²´ í•™ìŠµ ë°ì´í„°ë¥¼ í•œ ë²ˆì”© ëª¨ë‘ í•™ìŠµì‹œí‚¨ íšŸìˆ˜(iteration)
- Adam Optimizer: Momentumê³¼ AdaGrad/RMSPropì˜ ì´ì ì„ ì¡°í•©, Adaptive Learning Rateê°€ ì ìš©

### Dropout
- ì‹ ê²½ë§ì— ì ìš©í•  ìˆ˜ ìˆëŠ” íš¨ìœ¨ì ì¸ Overfitting íšŒí”¼ ë°©ë²• ì¤‘ í•˜ë‚˜
- Trainingì„ ì§„í–‰í•  ë•Œ ë§¤ Batch ë§ˆë‹¤ ë ˆì´ì–´ ë‹¨ìœ„ë¡œ ì¼ì • ë¹„ìœ¨ ë§Œí¼ì˜ ë‰´ëŸ°ì„ êº¼ëœ¨ë¦¼
- Test/Inference ë‹¨ê³„ì—ëŠ” Dropoutì„ ê±·ì–´ë‚´ì–´ ì „ì²´ ë‰´ëŸ°ì´ ì‚´ì•„ìˆëŠ” ì±„ë¡œ Inferenceë¥¼ ì§„í–‰
- ëœë¤í•˜ê²Œ ë‰´ëŸ°ì„ êº¼ëœ¨ë ¤ í•™ìŠµì„ ë°©í•´í•¨ìœ¼ë¡œì¨ ëª¨ë¸ì˜ í•™ìŠµì´ Training Dataì— í¸í–¥ë˜ëŠ” ê²ƒì„ ë°©ì§€
- ë™ì¼í•œ ë°ì´í„°ì— ëŒ€í•´ ë§¤ë²ˆ ë‹¤ë¥¸ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒê³¼ ê°™ì€ íš¨ê³¼ë¥¼ ë°œìƒì‹œì¼œ Model Ensemble íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ
- ê°€ì¤‘ì¹˜ ê°’ì´ í° íŠ¹ì • ë‰´ëŸ°ì˜ ì˜í–¥ë ¥ì´ ì»¤ì ¸ ë‹¤ë¥¸ ë‰´ëŸ°ë“¤ì˜ í•™ìŠµ ì†ë„ì— ë¬¸ì œë¥¼ ë°œìƒì‹œí‚¤ëŠ” Co-Adaptation íšŒí”¼

### Batch Normalization
- ì…ë ¥ê°’ì— Standardizationê³¼ ê°™ì€ Normalizationì„ ì ìš©í•˜ë©´ ì „ë°˜ì ìœ¼ë¡œ ëª¨ë¸ì˜ ì„±ëŠ¥ ì¦ê°€
- Normalizationì´ ì œëŒ€ë¡œ ì ìš©ë˜ì§€ ëª»í•  ê²½ìš° ìµœì ì˜ Cost ì§€ì ìœ¼ë¡œ ê°€ëŠ” ê¸¸ì„ ë¹ ë¥´ê²Œ ì°¾ì§€ ëª»í•¨
- Batch Normalizationì€ ì…ë ¥ê°’ ë¿ë§Œ ì•„ë‹ˆë¼ Hidden Layerì— ìˆëŠ” Inputì—ë„ Normalizationì„ ì ìš©
- Activation Functionì„ ì ìš©í•˜ê¸° ì „ì— Batch Normalizationì„ ë¨¼ì € ì ìš©
- Standardization ì ìš© ì‹œ Sigmoidì˜ ì„ í˜• ë¶€ë¶„ì—ì„œë§Œ ê°’ì´ ë‚˜ì™€ ë¹„ì„ í˜• í•¨ìˆ˜ì˜ ì—­í•  ìƒì‹¤
- Non-linearity: ì—´ì— `Î»`ë¥¼ ê³±í•˜ê³  `Î²`ë¥¼ ë”í•¨ (ì˜¤ì°¨ì—­ì „íŒŒë¥¼ í†µí•´ í•™ìŠµí•˜ëŠ” `Î¸`), Scaling ë° Shifting ì ìš©
- í•™ìŠµ ì†ë„ ë° í•™ìŠµ ê²°ê³¼ ê°œì„ , ê°€ì¤‘ì¹˜ ì´ˆê¸°ê°’ ì˜ì¡´ë„ê°€ ë‚®ìŒ, Overfitting ì–µì œ íš¨ê³¼, **í•™ìŠµ ì†ë„ í–¥ìƒì´ ì£¼ëª©ì **

---

`done`

---

http://playground.tensorflow.org/

spiral ì˜ˆì‹œ
https://goo.gl/vvea3e , https://goo.gl/yfLEHq , https://goo.gl/jqVgr9

---

12p
í´ë˜ìŠ¤ ë¶ˆê· í˜• - SMOTE

* Introducing the NVIDIA DPU @ https://j.mp/2GjQRBL
* Mythbusters Demo GPU vs CPU @ https://j.mp/3abWFb2
* Somethings about TPU (Google I/O) : https://goo.gl/JSTbDv
* êµ¬ê¸€, AIë¡œ AIì¹© ê°œë°œ ì‹œë„..."ì‚¬ëŒë³´ë‹¤ ë‚˜ì•„â€œ @ http://j.mp/2vbQ545 
* Colab-specific TPU examples (Fashion-MNIST, LSTM, BERT) @ https://goo.gl/DTTR31
* (CodeLab) TPU-speed data pipelines: tf.data.Dataset and TFRecords @ http://j.mp/2XF2Ccx
* Colossus Mk2 by Graphcore beating Nvidia's flagship A100 (UK startup)@ https://j.mp/3iig0Ke

39p
* Active Learning : ALiPy @ http://j.mp/2Q1EdYo
* model interpretability vs model explainability @ https://j.mp/2AUy4df 
* ë°ì´í„° ë¼ë²¨ë§ ë„ˆë¬´ ê·€ì°®ì•„ìš”: ì»¨ì„¼ì„œìŠ¤ ë¼ë²¨ë§ ë„ì…ê¸° (DEVIEW2020) @ https://j.mp/38AYUnC
* LIME (python library for model explanation & interpretable model) @ https://goo.gl/5JcZFY
* ì •í˜•ë°ì´í„°ë¥¼ ìœ„í•œ ì¸ê³µì‹ ê²½ë§ ëª¨ë¸, TabNet (Google Cloud AI, feature selection & interpretability) @ https://j.mp/38ABU8e

ëª¨ë¸ í•´ì„ ê°€ëŠ¥ì„± vs ì„¤ëª… ê°€ëŠ¥ì„±

active learning
10ë§Œì¥ ì¤‘ ë§Œì¥ì„ ì„ íƒí•´ ì •ë‹µì„ ë‹¬ ë•Œ ì–´ë–¤ ì• ë“¤ì„ ë‹¬ì•„ì¤„ì§€

41p
ì„ í˜•ëŒ€ìˆ˜, í–‰ë ¬ ì˜ í‘œí˜„í•˜ëŠ” ìœ íŠœë¸Œ >
Essence of linear algebra @ https://goo.gl/O79Wsx 
Essence of calculus @ https://goo.gl/3KHqk6

* 3Blue1Brown í•œêµ­ì–´ ë²ˆì—­ ì‹œë¦¬ì¦ˆ @ https://j.mp/3939TGO
* ìˆ˜í•™ì „ê³µìê°€ ì¶”ì²œí•˜ëŠ” ìœ íŠœë¸Œ ìˆ˜í•™ ê°•ì˜ ëª©ë¡ @ http://j.mp/2QiFDj5
* ë¨¸ì‹ ëŸ¬ë‹ & ë”¥ëŸ¬ë‹ ë¶„ì•¼ë³„ ì¶”ì²œ ì„œì  ë¦¬ìŠ¤íŠ¸ @ https://j.mp/3q1CGCV
* Statistics 110 : Probability from Harvard (ì»¤ë„¥íŠ¸ ì¬ë‹¨, í•œê¸€) @ https://j.mp/3uhcOVp
* Introduction to linear algebra for ML (Matrix Factorization, SVD, Embedding, ì¶”ì²œì‹œìŠ¤í…œ, PCA +@) @ https://j.mp/2UGa0EM
* Machine Learning Roadmap 2020 (ë¶„ì„ í”„ë¡œì„¸ìŠ¤ë³„ ë…¸í•˜ìš°/ì •ì˜/ìë£Œë§í¬ ë“±) @ https://j.mp/335sVeg & https://j.mp/3hMY9Li

* ë°ì´í„°ê³¼í•™ìì™€ ë°ì´í„°ì—”ì§€ë‹ˆì–´ë¥¼ ìœ„í•œ ì¸í„°ë·° ë¬¸ë‹µì§‘ (ì±…, ë§¤ìš° ì„¸ë¶€ì ì¸ MLDL ê´€ë ¨ Q&A) @ https://j.mp/2ZX7wAV

ë…¼ë¬¸ ìë£Œ
* PR12 (Youtube, ë…¼ë¬¸ì½ê¸°ëª¨ì„) @ https://j.mp/2FDR6Xz
* Papers You Must Read (by ê³ ë ¤ëŒ€ Data Science & Business Analytics ì—°êµ¬ì‹¤)@ https://j.mp/3259sIy -> ê³ ë ¤ëŒ€ ì„ì‚¬ ë“¤ì–´ì˜¤ìë§ˆì ì½ì–´ì•¼í•  ë…¼ë¬¸

45p
ëª¨ë‘ë¥¼ ìœ„í•œ ë¨¸ì‹ ëŸ¬ë‹/ë”¥ëŸ¬ë‹ ê°•ì˜

êµ¬ê¸€ ë¨¸ì‹ ëŸ¬ë‹ ë‹¨ê¸°ì§‘ì¤‘ê³¼ì •

ë¨¸ì‹ ëŸ¬ë‹ ìš©ì–´ì§‘ (Google ML Crash course) @ https://goo.gl/Q4bYbz

70p
* XOR solvable activation function helps DNNs @ http://j.mp/2ZSz0ar
* Dendritic action potentials and computation in human layer 2/3 cortical neurons @ http://j.mp/2ZQbnPC

http://playground.tensorflow.org/

* Deep learningì—ì„œì˜ Kernel trick, Feature crosses @ http://j.mp/2p5CbO2

* RAdam (Rectified-Adam) @ http://j.mp/2qzRjUa & http://j.mp/2N322hu
* AdaBound (using dynamic bound of learning rate) @ http://j.mp/2VAo3Lz
* AdaBelief (Adapting stepsizes by the belief in observed gradients) @ https://j.mp/3iQEBaL

* ONNX (Open Neural Network Exchange Format) @ https://goo.gl/nvJ8TH & https://goo.gl/GYJze7 + ONNX ì²«ê±¸ìŒ (eBook) @ https://j.mp/2ZLJwkK
* How to run PyTorch models in the browser with ONNX.js @ https://j.mp/3bP8qFf
ì˜¤ë‹‰ìŠ¤: ì¸ê³µì‹ ê²½ë§ ëª¨ë¸ì„ pytorchì—ì„œ tensorflow, kerasë¡œ ëŒê³ ì™€ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ

* ìš°ë¦¬ê°€ ë³¼ ìˆ˜ ì—†ëŠ” ë°ì´í„°ë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆì„ê¹Œ?(DEVIEW2020, Privacy preserving AI / PySyft / Federated Learning) @ https://j.mp/2LVNyD0

* ì–¼êµ´ íƒì§€ Android ì•± ê°œë°œ (TF Lite & ML Kit) @ https://j.mp/2SKkbaB
* TF Lite Model Maker Examples (ì´ë¯¸ì§€ë¶„ë¥˜, í…ìŠ¤íŠ¸ë¶„ë¥˜, BERT QA) @ https://j.mp/3xly26w
* On-Device ML Examples by Google (ì„¸ë¶€ ê³¼ì œë³„ Android/iOS/tf.js ì†ŒìŠ¤ ì½”ë“œ í¬í•¨) @ https://j.mp/3chnMDC

* Papers with Code: Methods (ë”¥ëŸ¬ë‹ ê´€ë ¨ ê¸°ë²• ë¶„ì•¼ë³„ ë…¼ë¬¸ & êµ¬í˜„ ëª¨ìŒ) @ https://j.mp/329FcxD
-> ì´ë¯¸ì§€ì— ìë§‰ ë“± ê³¼ì œ ë³„ë¡œ êµ¬í˜„ëœ ì½”ë“œ ëª¨ìŒ

* Deep Learning State of the Art (MIT, 2019) @ http://j.mp/2vhAkF3
* AI ê¸°ëŠ¥êµ¬í˜„ ì›¹ì‚¬ì´íŠ¸ & ìƒí™œ ì† AI ì‚¬ë¡€ ëª¨ìŒ @ https://j.mp/2XWYDpN

---

ì¸ê³µì§€ëŠ¥ ë¶„ì•¼
1. CV(Computer Vision)
2. NLP(ìì—°ì–´)
3. Audio
+ RL (ê°•í™”í•™ìŠµ)

Multi-modal

ì‚¬ë¬¼ ê²€ì¶œ
í›„ë³´ ì˜ì—­ ì¶”ì¶œ > CNN
ì´ë¯¸ì§€ë¥¼ í”½ì…€ ë‹¨ìœ„ë¡œ í•˜ë©´ ë„ˆë¬´ í¼ > CNNì´ ëŒ€ì•ˆ

CNN
ì´ë¯¸ì§€ì— (ì„¸íƒ€) í•„í„°(3x3) > ê°™ì€ ìœ„ì¹˜(í”½ì…€)ì— ìˆëŠ” ê²ƒë“¤ë¼ë¦¬ ê³±í•¨
ê³±í•´ì§„ ê°’ì„ ë”í•¨ > ìƒˆ ë„í™”ì§€ ìœ„ì— ë”í•œ ê°’ì„ ë„£ì–´ì¤Œ
í•„í„°ë¥¼ í•œì¹¸ì”© ì´ë™ì‹œí‚¤ë©° ë”í•œ ê°’ìœ¼ë¡œ ë„í™”ì§€ ì±„ì›€ > ì´ë¯¸ì§€ì˜ íŠ¹ì§•ì„ ì°¾ì•„ëƒ„
í•„í„° ê²°ê³¼ ë„í™”ì§€ë¥¼ Feature Map
ì„œë¡œ ë‹¤ë¥¸ íŠ¹ì§•ì„ ì¡ì•„ë†“ì€ ì´ë¯¸ì§€ê°€ 64ì¥ > í”½ì…€ì„ ì¤„ì—¬ì¤Œ
<<< Convolutional Layer

Pooling Layer >>>
Feature Mapì„ ê·¸ëŒ€ë¡œ ê°€ì ¸ê°€ì§€ ì•Šê³  4ì¹¸ ì§€ì •í•´ì„œ ìµœëŒ“ê°’ë§Œ ì‚´ë¦¼
Max(Average)-Pooling

í•˜ë‚˜ì˜ ì„¸íŠ¸ë¡œ êµ¬ì„±ëœ ë ˆì´ì–´

CPR -> FC(Fully-Connected == Dense) -> Output Layer

Transpool Learning í•  ë•Œ FC ì „ê¹Œì§€ë§Œ ê°€ì ¸ì˜´
-> 

3x3: í•„í„° ì‚¬ì´ì¦ˆ (ì§€ì •)
í•„í„°ë¥¼ ì´ë™ì‹œí‚¤ëŠ” ì‘ì—…: ìœˆë„ìš° ìŠ¬ë¼ì´ë”©
ì´ë¯¸ì§€ ë°”ê¹¥ìª½ì— 0ì„ ì±„ìš°ê³  í•„í„° >
ì›ë³¸ì´ë‘ feature mapì˜ í¬ê¸°ê°€ ë™ì¼í•˜ê²Œ ìœ ì§€: zero padding

## CNN ë§í¬
https://yceffort.kr/2019/01/29/pytorch-3-convolutional-neural-network  (ìƒë‹¨ ì´ë¯¸ì§€) 
https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1 (ì¤‘ë°˜ë¶€ í•„í„° ì´ë¯¸ì§€)
http://taewan.kim/post/cnn/ (í•„í„° ê³„ì‚° ì˜ˆì‹œ, RGB ì±„ë„ ì ìš© ì˜ˆì‹œ, Pooling ê³„ì‚° ì˜ˆì‹œ <- ì‰¬ìš´ í•œê¸€ ì„¤ëª…)
https://setosa.io/ev/image-kernels/ (í•„í„°ë³„ ì ìš© ê²°ê³¼ ì˜ˆì‹œ)

https://poloclub.github.io/cnn-explainer/ (í•„í„° & í’€ë§ êµ¬ì²´ì ì¸ ê³„ì‚° ì˜ˆì‹œ <- êµ¬ì²´ì ì¸ ê³„ì‚°)
https://www.slideshare.net/yongho/ss-79607172 (ê°œë… ì´í•´ìš© ìë£Œ)
https://www.youtube.com/playlist?list=PLl1irxoYh2wzOOU9hvJqMYc215wAlxrpp (ê°œë… ì´í•´ìš© ìë£Œ)

CNN ë„¤ëª¨ê°€ í¬ê¸° ë•Œë¬¸ì— í”½ì…€ ë‹¨ìœ„ë¡œ
-> Image Segmentation

HuggingFace (Pytorch, Tensorflow)

BERT

* í•œêµ­ì–¸ë¡ ì§„í¥ì¬ë‹¨, ë¹…ì¹´ì¸ì¦ˆ ê¸°ì‚¬ ê¸°ë°˜ AI ì–¸ì–´ëª¨ë¸ â€˜KPF-BERTâ€™ ê³µê°œ (2022) @ https://bit.ly/35uBdQC
* ê¸ˆìœµ ì–¸ì–´ ì´í•´ë¥¼ ìœ„í•´ ê°œë°œëœ ALBERT í†ºì•„ë³´ê¸° with Transformers (KB-ALBERT, PyCon Korea 2020) @ https://j.mp/2G3y0dO

GPT
Transformer ëª¨ë¸
https://blog.naver.com/PostView.nhn?blogId=krugmanpaul&logNo=222218829812
í…ìŠ¤íŠ¸ê°€ ë“¤ì–´ì˜¤ë©´ ë‚´ë¶€ì ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ë²ˆì—­ëœ ê²°ê³¼ ë‚´ë³´ëƒ„
í¬ê²Œ ë‘ìª½(ì „ë°˜ë¶€: encoder, í›„ë°˜ë¶€: decoder)
encoder: ë¬¸ì¥ì„ ì´í•´í•˜ê¸° ìœ„í•´ í•™ìŠµ
decoder: ì´í•´í•œ ë¬¸ì¥ì„ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì¥ ìƒì„±

* The Illustrated GPT-2 (Visualizing Transformer Language Models) @ http://j.mp/2Mx4f74 + GPT-2-simple @ http://j.mp/32DOhN8
* KoGPT(KakaoBrain) @ https://j.mp/3qXZpmL + KoGPT2 (SKT) @ http://j.mp/3bp5378 & http://j.mp/2uzUxJv & https://j.mp/3aYYTct

---------------------------------------------------------
ğŸ”¥ PMì´ ëì—†ì´ ì„±ì¥í•˜ëŠ” ë²•
<Create Your Growth with Endless Possibilities>

ì—°ì‚¬ë‹˜: íŒŒë¦¬ SAP PM ê¹€ì˜ìš±ë‹˜ (24ë…„ì—¬ PM ê²½ë ¥)
ì¼ì‹œ: 2022ë…„ 4ì›” 30ì¼ (í† ìš”ì¼) ì˜¤í›„ 8ì‹œ ~ 9ì‹œë°˜

ì‚¬ì „ ì„¤ë¬¸: https://abit.ly/pm02form
ì¥ì†Œ: zoom  https://abit.ly/pm02zoom 


* Awesome GPT-3 (a collection of demos and articles about the OpenAI GPT-3 API) @ https://j.mp/2ELqjbv + GPT Crush @ https://gptcrush.com/
* GPT-3 based Text2Code examples : (React-app) @ https://j.mp/2WALBhN / (LaTeX) @ https://j.mp/2WBnRdA / (JSX) @ https://j.mp/3jiKsoX
* (new & advanced) Screenshot-to-code @ https://goo.gl/tmMJSj & https://goo.gl/xKoQaw + AlphaCode Attention Visualization @ https://bit.ly/3rneqyf

VSCode
Serenade Voice ë§í•˜ë©´ ì½”ë“œë¥¼ ì§¤ ìˆ˜ ìˆê²Œ

Github Copilot

* OpenAI Codex (the model that powers GitHub Copilot) @ https://j.mp/3tP1qB5
* (OpenAI) DALLÂ·E 2 @ https://t.ly/LjMf

* ì„¸ìƒì— ì—†ëŠ” ì‚¬ëŒë“¤ - ì¸ê°„ì„ ëŒ€ì²´í•  ë””ì§€í„¸ ì¸ê°„ @ https://j.mp/3hHgxos

* AI-Colorized Korea, 1900 @ https://bit.ly/37xJJzj
* AI-Colorized Atomic Bomb Cannon test @ https://j.mp/3k7dTK2

* U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation @ https://j.mp/3hP5xWa

* ë„¤ì´ë²„ì›¹íˆ°, AI ìë™ì±„ìƒ‰ ì„œë¹„ìŠ¤ ì¶œì‹œ @ https://j.mp/3q2Gc2Q

* Nvidia Inpainting Demo @ https://bit.ly/3rL5LWe
* Adobe Remove objects with Content-Aware Fill @ https://j.mp/2HiB8mx
* Photoshopâ€™s AI neural filters can tweak age and expression with a few clicks @ https://j.mp/34iPInc

* Super resolution êµ¬í˜„ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ Image Super-Resolution (ISR)@ http://j.mp/2PPCwOR

* A Deeper Look Into The Life of An Impressionist (deepfakes of celebrities) @ http://j.mp/2NsDHll
* Fake video of President Barack Obama @ https://goo.gl/xU1jTc + Very realistic Tom Cruise Deepfake @ https://j.mp/3zO6JTH

-------------------------------------------------------------------------------
* 20 different ways AI could be used by criminals over the next 15 years @ https://j.mp/2EYL3ML
* Thieves are now using AI deepfakes to trick companies into sending them money @ http://j.mp/2PB8MpQ
* How to recognize fake AI-generated images @ http://j.mp/2H5NNqi & http://j.mp/2H7C7DE & http://j.mp/2H5RFru
* Fakebox (Detector for fake news) @ https://machinebox.io/docs/fakebox

* Soundraw (AI Music Generator for creators) @ https://j.mp/3biJhTH
* OpenAI Jukebox (genre/artist/lyrics -> generates music with singing & raw audio, PyTorch) @ https://j.mp/2W94HvN

Soundraw: ì €ì‘ê¶Œ íšŒíŒŒí•˜ê¸° ìœ„í•´ ë¹„ìŠ·í•œ ìŒì•…ì„ ì‘ê³¡

* ë¦½ì‹±í¬ë§Œìœ¼ë¡œ ë§ ì•Œì•„ë“£ëŠ” AI, ìŒì„±ì¸ì‹ ë„˜ì–´ ë¬´ì„±ì¸ì‹ @ https://j.mp/2GNOx67
* ë¨¸ìŠ¤í¬ì˜ ë‡Œê³¼í•™ ìŠ¤íƒ€íŠ¸ì—… 'ë‰´ëŸ´ë§í¬', ìƒê°ë§Œìœ¼ë¡œ ë¹„ë””ì˜¤ ê²Œì„ í•˜ëŠ” ì›ìˆ­ì´ ê³µê°œ @ https://j.mp/3ebOduA
* Neural networks translated a paralyzed manâ€™s brainwaves into conversational phrases @ https://j.mp/3wTRMgh

* FastAutoAugment by Kakao brain @ https://bit.ly/3K8wRga



Albumentations ë¼ì´ë¸ŒëŸ¬ë¦¬
Augment ì‘ì—… í¸í•˜ê²Œ

 Albumentations (fast augmentations based on highly-optimized OpenCV library) @ https://goo.gl/GYKK4y & https://j.mp/2J7Enej

 CutMix (data augmentation method proposed by Clova AI Research) @ http://j.mp/2Q0hBaA & http://j.mp/2WhJSjD
* Test Time Augmentation and how to perform it with Keras @ http://j.mp/2NtlVli & http://j.mp/2NqVVXH (+ http://j.mp/2Nnu8an)


The Batch
Andrew Ng ë‰´ìŠ¤ë ˆí„°
ì „ì„¸ê³„ì—ì„œ í•«í•œ ì¸ê³µì§€ëŠ¥ ê´€ë ¨ ì´ìŠˆ ì „ë‹¬


---

ì‹¤ìŠµ 5-1

Conv2D -> Condulutional Layer 2D
input_shape: í•œ ì¥ ì´ë¯¸ì§€ì— ëŒ€í•œ ì°¨ì›
flatten ì“°ë©´ input_shapeê°€ í•„ìš” ì—†ìŒ
ì»¬ëŸ¬ ì´ë¯¸ì§€: (28,28,3) -> 3ì±„ë„
(3,3) -> kernel size
(kernel_regularizer -> L1,L2 ê·œì œ)

* tf.keras preprocessing layers (with examples) @ https://j.mp/3ksCIl8

* Label smoothing, When Does Label Smoothing Help? @ https://j.mp/337naLv & https://j.mp/3lYi1Oq

`validation_data=test_generator`
- ë°ì´í„°ê°€ ë‹¤ ë‚˜ëˆ ì ¸ ìˆë‹¤ë©´ train dataëŠ” ë†”ë‘ê³  validation_dataë¥¼ ë”°ë¡œ ë„£ì–´ì¤Œ


