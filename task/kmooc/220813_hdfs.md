## [과제] 빅데이터 프레임워크
- [mapreduce 프로그램 작성](#mapreduce-프로그램-작성)
- [mapreduce 결과](#mapreduce-결과)
- [과정 수행 절차](#과정-수행-절차)

<br>

> Mapreduce를 이용해 단어 개수를 세는 어플리케이션을 작성하여   
> 다음 주어진 텍스트 파일 ( report.txt)에서 각 단어가 몇 개 있는지를 확인하고,    
> 이를 출력한 결과를 작성하여라.

<br>

### mapreduce 프로그램 작성

```bash
# report.txt

Apple Apple Apple
Banana Banana
Melon
```

```python
# mapper.py

import sys
for line in sys.stdin:
    line = line.strip()
    keys = line.split()
    for key in keys:
        value = 1
        print("{0}\t{1}".format(key,value))
```

```python
# reducer.py

import sys
last_key=None
running_total=0

for input_line in sys.stdin:
    input_line = input_line.strip()
    this_key,value = input_line.split("\t",1)
    value = int(value)

    if last_key == this_key:
        running_total += value
    else:
        if last_key:
            print("{0}\t{1}".format(last_key,running_total))
        running_total = value
        last_key = this_key

if last_key == this_key:
    print("{0}\t{1}".format(last_key, running_total))
```

### mapreduce 결과

```python
# /user/wordcount/part-00000

Apple   3
Banana  2
Melon   1
```

### 과정 수행 절차

1. Hadoop Docker 실행

```bash
docker compose up
```

2. namenode 컨테이너 진입

```bash
docker exec -it namenode /bin/bash
```

3. `report.txt` 생성 및 HDFS 디렉토리 상에 이동

```bash
cd hadoop-data
vi report.txt
hdfs dfs -put input.txt /user/hduser
```

4. mapreduce 프로그램 작성 후 실행

`hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar -input /user/hduser/report.txt -output /user/wordcount -mapper mapper.py -file mapper.py -reducer reducer.py -file reducer.py`

5. mapreduce 결과 확인

```bash
hdfs dfs -cat /user/wordcount/part-00000
```
