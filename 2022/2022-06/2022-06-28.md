# NLP Study

---

## 02-01. Tokenization
- Corpus에서 token이라 불리는 단위로 나누는 작업
- 단어 토큰화에서 단순히 구두점이나 특수문자를 제거하는 것은 의미의 손실을 발생시킬 수 있기 때문에,   
  사용자의 목적과 일치하는 토큰화 도구를 사용할 필요가 있음
- **구두점이나 특수 문자가 필요한 경우**: Ph.D, AT&T, $45.55, 01/02/06 등
- **줄임말과 단어 내에 띄어쓰기가 있는 경우**: what're/what are, New York, rock 'n' roll 등
- 문장 토큰화에서 단순히 마침표를 기준으로 문장을 잘라내는 것은
  192.168.56.31, gmail.com과 같은 경우를 고려했을 때 올바르지 않음

### 한국어 토큰화
- 한국어의 경우 띄어쓰기가 가능한 단위가 어절인데,   
  '그가', '그에게', '그를'과 같이 어절이 독립적인 단어로 구성되는 것이 아니라   
  조사 등의 무언가가 붙어있는 경우가 많기 때문에 이를 전부 형태소 단위로 분리해줘야 함
- **자립 형태소**: 접사, 어미, 조사와 상관업싱 자립하여 사용할 수 있는 형태소, [체언, 수식언, 감탄사] 등
- **의존 형태소**: 다른 형태소와 결합하여 사용되는 형태소, [접사, 어미, 조사, 어간]
- 한국어의 경우 **띄어쓰기**가 지켜지지 않아도 글을 쉽게 이해할 수 있어 띄어쓰기가 잘 지켜지지 않음
- **품사 태깅**: 단어의 의미를 제대로 파악하기 위해서는 해당 단어가 어떤 품사로 쓰였는지 구분할 필요

### NLTK, KoNLPy

```python
from nltk.tokenize import word_tokenize # 단어 토큰화
from nltk.tag import pos_tag # 품사 태깅
```

```python
from konlpy.tag import Okt
okt = Okt()
okt.porphs(sentence) # 형태소 추출
okt.pos(sentence) # 품사 태깅
```

## 02-02. Cleaning and Normalization
- **Cleaning(정제)**: 갖고 있는 corpus로부터 노이즈 데이터를 제거
- **Normalization(정규화)**: 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만듦
- 영어권 언어에서 단어의 개수를 줄이는 정규화 방법으로 **대,소문자 통합**을 활용
- **노이즈 데이터**: 아무 의미 없는 특수 문자 등, 분석하고자 하는 목적에 맞지 않는 불필요한 단어들
- 불필요한 단어를 제거하기 위해 **불용어**, **등장 빈도가 적은 단어**, **길이가 짧은 단어** 등을 제거
- 노이즈 데이터의 특징을 잡아낼 수 있다면, **정규표현식**을 사용해서 제거

## 02-03. Stemming and Lemmatization

### Lemmatization
- **Lemma(표제어)**: 기본 사전형 단어, [am, are, is]의 뿌리 단어 be 등
- **Stem(어간)**: 단어의 의미를 담고 있는 단어의 핵심 부분, 'cats'에서 'cat'
- **Affix(접사)**: 단어에 추가적인 의미를 주는 부분, 'cats'에서 's'

```python
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
lemmatizer.lemmatize(word)
# am -> be, having -> have
```

### Stemming

```python
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
stemmer.stem(word)
# am -> am, having -> hav
```

### 한국어에서의 어간 추출
- 5언 9품사의 구조에서 용언에 해당되는 동사와 형용사는 어간과 어미의 결합으로 구성
- **활용**: 용언의 어간이 어미를 가지는 일
- **규칙 활용**: 어간이 어미를 취할 때 어간의 모습이 일정, `잡/어간 + 다/어미`
- **불규칙 활용**: 어간이 어미를 취할 때 어간의 모습이 바뀌거나 특수한 어미일 경우, '오르+아/어->올라' 등

## 02-04. Stopword
- **Stopword(불용어)**: 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 기여하지 않는 단어, [조사, 접미사] 등

```python
from nltk.corpus import stopwords
stop_words_list = stopwords.words('english')
```

```python
okt = Okt()
okt.morphs(sentence) # 조사, 접속사 등 제거
# 또는 불용어 사전을 만들어서 제거
```

## 02-05. Regular Expression
- 정규 표현식 [참고](https://wikidocs.net/21703)

## 02-06. Integer Encoding
- 컴퓨터는 텍스트보다 숫자를 더 잘 처리할 수 있기 때문에 텍스트를 숫자로 변경
- 단어를 빈도수 순으로 정렬하고 순서대로 낮은 숫자부터 정수를 부여
- dictionary, Counter, nltk.FreqDist, keras.Tokenizer 등 활용

```python
from nltk import FreqDist
FreqDist(np.hstack(preprocessed_sentences)) # np.hastack으로 문장 구분을 제거
```

```python
from tensorflow.keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer() # num_words 파라미터로 사용할 단어 개수 지정
tokenizer.fit_on_texts(preprocessed_sentences) # 빈도수 기분으로 단어 집합 생성
print(tokenizer.word_intex) # 정수 인덱스 확인
print(tokenizer.word_counts) # 단어 빈도수 확인
print(tokenizer.texts_to_sequences(preprocessed_sentences)) # corpus를 인덱스로 변환
```

## 02-07. Padding
- 병렬 연산을 위해 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업이 필요

```python
from tensorflow.keras.preprocessing.sequence import pad_sequences
encoded = tokenizer.texts_to_sequences(preprocessed_sentences)
padded = pad_seqences(encoded)
# padding='post'를 입력해야 뒤에서 부터 0을 채움
# maxlen으로 문장 길이 조절
# truncating='post'를 통해 문장 길이 초과 시 뒤의 단어가 삭제되도록 설정
```

## 02-08. One-Hot Encoding
- **Vocabulary(단어 집합)**: 서로 다른 단어들의 집합, book과 books과 같은 변형 형태도 다른 단어로 간주
- **One-Hot Encoding**: 단어 집합의 크기를 벡터의 차원으로 하고,   
  표현하고 싶은 단어에 1, 다른 인텍스에 0을 부여하는 단어의 벡터 표현 방식
- 단어의 개수가 늘어날 수록 벡터를 저장하기 위해 필요한 공간이 계속 늘어나는 단점
- 단어의 유사도를 표현하지 못하는 단점 (강아지, 개, 냉장고 등)

```python
from tensorflow.keras.utils import to_categorical
encoded = tokenizer.texts_to_sequences(preprocessed_sentences)[0]
one_hot = to_categorical(encoded)
```

## 03-01. Language Model
- **Language Model** : 단어 시퀀스(문장)에 확률을 할당하는 모델, 이전 단어들이 주어졌을 때 다음 단어를 예측
- 단어 시퀀스 W의 확률 $P(W)=P(w_1,w_2,w_3,w_4,w_5,...,w_n)$
- 다음 단어 등장 확률 $P(w_n|w_1,...,w_{n-1})$

## 03-02. Statistical Language Model

### 조건부 확률
||남학생(A)|여학생(B)|계|
|:-:|:-:|:-:|:-:|
|중학생(C)|100|60|160|
|고등학생(D)|80|120|200|
|계|180|180|360|

- 학생을 뽑았을 때, 고등학생이면서 남학생일 확률 $P(A \bigcap B)=80/360$
- 고등학생 중 한명을 뽑았을 때, 남학생일 확률 $P(A|D)=P(A \bigcap D)/P(D)=(80/360)/(200/360)$

### 문장에 대한 확률
- 'An adorable little boy is spreading smiles'의 확률   
  $P(\text{An adorable little boy is spreading smiles})=\\
  P(\text{An}) \times P(\text{adorable}|\text{An}) \times ... \times P(\text{smiles}|\text{An adorable little boy is spreading})$

### 카운트 기반 접근
- An adorable little boy가 100번 등장했을 때 그 다음에 is가 등장한 경우가 30번이라면,   
  $P(\text{is}|\text{An adorable little boy})$는 30%
- 카운트 기반으로 훈련할 경우 단어 시퀀스가 없어 확률이 0이 되는 경우를 방지하기 위해 방대한 양의 훈련 데이터가 필요
- **회소 문제**: 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제

$$P(\text{is}|\text{An adorable little boy})= \frac{count(\text{An adorable little boy is})}{count(\text{An adorable little boy})}$$

## 03-03. N-gram Language Model
- 통계적 언어 모델의 일종이지만, 모든 단어가 아닌 일부 단어만 고려하는 접근 방법 사용
- An adorable little boy에서 is가 나올 확률을 boy가 나왔을 때 is가 나올 확률로 대체   
  $P(\text{is}|\text{An adorable little boy}) \approx P(\text{is}|\text{boy})$
- 뒤의 단어 몇 개만 보다 보니 의도하고 싶은 대로 문장을 끝맺음하지 못하는 경우가 발생
- 전체 문장을 고려한 언어 모델보다는 정확도가 떨어짐
- 몇 개의 단어를 볼지 n을 정하는 것은 trade-off 문제를 발생시킴, n은 최대 5를 넘게 잡아서는 안된다고 권장

### N-gram
- **n-gram**: n개의 연속적인 단어 나열
- An adorable little boy에 대해   
  unigrams: an, adorable, little, boy   
  bigrams: an adorable, adorable little, little boy

## 03-05. Perplexity
- **Perplexity(PPL)**: 헷갈리는 정도, 낮을수록 언어 모델의 성능이 좋음

$$PPL(W)=P(w_1,w_2,w_3,...,w_N)^{-\frac{1}{N}}=\sqrt[N]{\frac{1}{P(w_1,w_2,w_3,...,w_N)}}$$

### Branching Factor
- **Branching factor(분기계수)**: PPL이 선택할 수 있는 가능한 경우의 수
- 대해 PPL이 10이 나왔을 때, 언어 모델은 테스트 데이터에 대해 다음 단어를 예측할 때 평균 10개의 단어를 고려
- PPL의 값이 낮다는 것은 테스트 데이터 상에서 높은 정확도를 보이는 것일뿐, 반드시 사람이 직접 느끼기에 좋은 모델인 것은 아님

$$PPL(W)=P(w_1,w_2,w_3,...,w_N)^{-\frac{1}{N}}=(\frac{1}{10}^N)^{-\frac{1}{N}}=\frac{1}{10}^{-1}=10$$

## 04-01. 단어의 표현 방법
- **국소 표현(이산 표현)**: 해당 단어 그 자체만 보고, 특정값을 맵핑하여 단어를 표현하는 방법
- **분산 표현(연속 표현)**: 그 단어를 표현하고자 주변을 참고하여 단어를 표현하는 방법

## 04-02. Bag of Words
- Bag of Words(BoW): 단어들의 순서는 고려하지 않고, 단어들의 출현 빈도에만 집중하는 텍스트 데이터 수치화 표현 방법

```python
doc1 = '정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.'
vocabulary : {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}
bag of words vector : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1]
```

```python
from sklearn.feature_extraction.text import CounteVectorizer
vector = CounterVectorizer() # stop_words 파라미터로 불용어 제거('english' 또는 리스트 등)
print('bag of words vector:', vector.fit_transform(corpus).toarray())
print('vocabulary:', vector.vocabulary_)
```

## 04-03. Document-Term Matrix
- **Docuemnt-Term Matrix(DTM)**: 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것
- One-hot vector와 마찬가지로 대부분의 값이 0인 희소 표현의 문제 발생
- 불용어와 중요한 단어에 대해서 가중치를 주기 위해 TF-IDF를 사용

## 04-04. TF-IDF
- **TF-IDF**: 단어의 빈도와 역 문서 빈도를 사용하여 DTM 내의 각 단어들마다 중요도를 가중치로 부여하는 방법
- $tf(d,t)$: 특정 문서 $d$에서의 특정 단어 $t$의 등장 횟수, DTM에서의 각 단어들의 가진 값
- $df(t)$: 특정 단어 $t$가 등장한 문서의 수, 특정 단어가 각 문서에서 등장한 횟수는 무시
- $idf(d,t)$: $df(t)$에 반비례하는 수,   
  총 문서의 수 n이 커질수록 기하급수적으로 증가하는 것을 방지하기 위해 $log$(일반적으로 자연 로그) 적용

$$idf(d,t)=log(\frac{n}{1+df(t)})$$

```python
from sklearn.feature_extraction.text import TfidfVectorizer
print(vector.fit_transform(corpus).toarray())
print(ve)
```
