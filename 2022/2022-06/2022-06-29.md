# NLP Study

---

## 09-01. Word Embedding

### Sparse Representation
- 벡터 또는 행렬의 값이 대부분 0으로 표현되는 방법, one-hot vector 등
- 단어의 개수가 늘어나면 벡터의 차원이 한없이 커지는 문제, 공간적 낭비 발생

### Dense Representation
- 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤 (0과 1뿐 아니라 실수 포함)

### Word Embedding
- 단어를 밀집 벡터의 형태로 표현하는 방법
- **Embedding Vector**: 워드 임베딩 과정을 통해 나온 결과
- LSA, Word2Vec, FastText, Glove 등

## 09-02. Word2Vec

### Distributed Representation
- 희소 표현을 다차원 공간에 벡터화하는 방법
- 분산 표현 방법은 분포 가설이라는 가정 하에 만들어진 표현 방법   
- **분포 가설**: 비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다
	(귀엽다, 예쁘다, 애교 등의 단어가 주로 함께 등장할 경우 벡터화 시 유사한 벡터값을 가짐)
- 저차원에 단어의 의미를 여러 차원에다가 분산하여 표현하기 대문에 단어 벡터 간 유의미한 유사도 계산 가능
- Word2Vec의 학습 방식으로 CBOW(Continuous Bag of Words)와 Skip-Gram이 존재

### CBOW
- 주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측하는 방법
- 중심 단어를 기준으로 window size만큼 앞뒤로 단어를 확인 (2n개의 단어 확인)
- **Sliding Window**: window를 옆으로 움직여서 주변 단어와 중심 단어의 선택을 변경해가며 학습 데이터셋 생성
- CBOW의 인공 신경망은 주변 단어들의 one-hot vector를 입력으로 중간 단어의 one-hot vector를 예측
- Word2Vec은 은닉층이 1개이며, 활성화 함수 없이 룩업 테이블이라는 연산을 담당하는 projection layer로 불림
- 입력층과 투사층 사이의 가중치 W는 ${V}\times{M}$ 행렬, 투사층에서 출력층 사이의 가중치 W'는 ${M}\times{V}$ 행렬
- W와 W'는 동일한 행렬의 전치가 아니라 서로 다른 행렬이기 대문에 CBOW는 W와 W'를 학습해가는 구조를 가짐
- 입력 벡터와 가중치 W 행렬의 곱은 W 행렬의 i번째 행을 그대로 읽어오는 것(lookup)과 동일
- 주변 단어의 one-hot vector와 가중치 W를 곱한 결과 벡터들은 투사층에서 만나 평균인 벡터를 구함
- 평균 벡터는 두 번째 가중치 행렬 W'와 곱해져서 one-hot vector들과 차원이 V로 동일한 벡터가 나옴
- 해당 벡터에 softmax 함수를 거쳐 다중 클래스 분류 문제를 위한 score vector를 생성
- score vector의 j번째 인덱스가 가진 값은 j번째 단어가 중심 단어일 확률로,   
	score vector $\hat{y}$와 중심 단어의 one-hot vector $y$의 오차를 줄이기 위해 cross-entropy 함수 사용

### Skip-gram
- CBOW와 반대로 중심 단어에서 주변 단어를 예측
- 중심 단어만을 입력으로 받기 때문에 투사층에서 벡터들의 평균을 구하는 과정은 없음
- 전반적으로 Skip-gram이 CBOW보다 성능이 좋다고 알려짐

### NNLM vs Word2Vec
- NNLM은 다음 단어를 예측하는 목적이지만, Word2Vec(CBOW)은 중심 단어를 예측하는 목적,
	때문에 NNLM이 이전 단어들만 참고한다면, Word2Vec은 예측 단어의 전후 단어들을 모두 참고
- Word2Vec은 NNLM에 존재하던 활성화 함수가 있는 은닉층을 제거하여 학습 속도에서 강점을 가짐

## 09-03. [Word2Vec 실습](https://wikidocs.net/50739)
- [위키피디아 실습](https://wikidocs.net/152606)

```python
from gensim.models import Word2Vec
from gensim.models import KeyedVectors

model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0)
```
- `size`: 워드 벡터의 특징 값, 임베딩된 벡터의 차원
- `window`: context window size
- `min_count`: 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 무시)
- `workers`: 학습을 위한 프로세스 수
- `sg`: 0은 CBOW, 1은 Skip-gram

```python
# 입력한 단어에 대해서 가장 유사한 단어들을 출력
model_result = model.wv.most_similar("man")
print(model_result)

[('woman', 0.842622697353363), ('guy', 0.8178728818893433), ('boy', 0.7774451375007629), ('lady', 0.7767927646636963), ('girl', 0.7583760023117065), ('gentleman', 0.7437191009521484), ('soldier', 0.7413754463195801), ('poet', 0.7060446739196777), ('kid', 0.6925194263458252), ('friend', 0.6572611331939697)]
```

```python
model.wv.save_word2vec_format('eng_w2v') # 모델 저장
loaded_model = KeyedVectors.load_word2vec_format("eng_w2v") # 모델 로드
```

```python
# 사전 훈련된 Word2Vec 임베딩
urllib.request.urlretrieve("https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz",
							filename="GoogleNews-vectors-negative300.bin.gz")
word2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)
# shape(3000000, 300)
```

```python
# 두 단어의 유사도 계산
print(word2vec_model.similarity('this', 'is')) # 0.407970363878
```

## 09-04. Negative Sampling
- Word2Vec이 학습 과정에서 전체 단어 집합이 아니라 일부 단어 집합에만 집중할 수 있도록 하는 방법
- 중심 단어에 대해 무작위로 선택된 일부 단어 집합에 대해 긍정 또는 부정을 예측하는 이진 분류 수행
- 전체 단어 집합의 크기만큼 선택지를 두고 다중 클래스 분류 문제를 푸는 Word2Vec보다 효율적인 연산

### SGNS
- 네거티브 샘플링을 사용하는 Skip-gram은 중심 단어와 주변 단어가 모두 입력이 되고,   
  두 단어가 실제로 윈도우 크개 내에 존재하는 이웃 관계인지 확률을 예측
- 중심 단어에 대한 라벨로 주변 단어를 사용하지 않고,   
  중심 단어와 주변 단어에 대한 이웃 관계를 표시하기 위한 라벨로 1 또는 0을 사용

### [SGNS 구현](https://wikidocs.net/69141)
- 20newsgroups 데이터 사용

```python
# 네거티브 샘플링 데이터셋 생성
from tensorflow.keras.preprocessing.sequence import skipgrams

skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded]

# (commited (7837), badar (34572)) -> 0
# (whole (217), realize (1036)) -> 1
# (reason (149), commited (7837)) -> 1
```

```python
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Embedding, Reshape, Activation, Input
from tensorflow.keras.layers import Dot
```

```python
embedding_dim = 100

# 중심 단어를 위한 임베딩 테이블
w_inputs = Input(shape=(1, ), dtype='int32')
word_embedding = Embedding(vocab_size, embedding_dim)(w_inputs)

# 주변 단어를 위한 임베딩 테이블
c_inputs = Input(shape=(1, ), dtype='int32')
context_embedding  = Embedding(vocab_size, embedding_dim)(c_inputs)
```

```python
# 두 임베딩 테이블에 대한 내적의 결과로 1 또는 0을 예측하기 위해 시그모이드 함수 사용
dot_product = Dot(axes=2)([word_embedding, context_embedding])
dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)
output = Activation('sigmoid')(dot_product)

model = Model(inputs=[w_inputs, c_inputs], outputs=output)
model.summary()
model.compile(loss='binary_crossentropy', optimizer='adam')
```

```python
# 5epochs 학습
for epoch in range(1, 6):
    loss = 0
    for _, elem in enumerate(skip_grams):
        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')
        second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')
        labels = np.array(elem[1], dtype='int32')
        X = [first_elem, second_elem]
        Y = labels
        loss += model.train_on_batch(X,Y)  
    print('Epoch :',epoch, 'Loss :',loss)
```

## 09-05. GloVe
- 카운트 기반과 예측 기반을 모두 사용하는 방법론으로, LSA와 Word2Vec의 단점 보완
- LSA는 단어의 빈도수를 차원 축소하여 잠재된 의미를 끌어내지만, 같은 단어 의미의 유추 작업 성능은 떨어짐
- Word2Vec는 예측 기반으로 단어 간 유추 작업에는 LSA보다 뛰어나지만,   
  window size 내 주변 단어만을 고려하여 전체적인 통계 정보를 반영하지 못함
- 임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것이 목적

### [Window based Co-occurrence Matrix](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture02-wordvecs2.pdf)
- 행과 열을 전체 단어 집합의 단어들로 구성하고,   
  i 단어의 window size 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬

### Co-occurence Probability
- 동시 등장 확률 $P(k|i)$는 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고,   
  특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률

```python
# pip install glove_python_binary

from glove import Corpus, Glove

corpus = Corpus() 

# 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성
corpus.fit(result, window=5)
glove = Glove(no_components=100, learning_rate=0.05)

glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)
glove.add_dictionary(corpus.dictionary)

print(glove.most_similar("man"))

[('woman', 0.9621753707315267), ('guy', 0.8860281455579162), ('girl', 0.8609057388487154), ('kid', 0.8383640509911114)]
```

## 09-06. FastText
- Word2Vec가 단어를 쪼개질 수 없는 단위로 생각한다면,   
  FastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주
- 각 단어를 글자 단위 n-gram의 구성으로 취급하여 n에 따라 단어들이 얼마나 분리되는지 결정
- tri-gram의 경우 apple에 대해서 [<ap, app, ppl, ple, le>]로 분리된 벡터 생성   
  (<, >는 시작과 끝을 의미)
- 내부 단어들을 Word2Vec로 벡터화하고 apple의 벡터값은 내부 단어의 벡터값들의 총 합으로 구성

### Out Of Vocabulary
- FastText는 데이터셋만 충분하다면 내부 단어를 통해 모르는 단어에 대해서도 유사도 계산 가능
- birthplace를 학습하지 않은 상태라도, birth와 place라는 내부 단어가 있다면 벡터를 얻을 수 있음

### Rare Word
- Word2Vec는 등장 빈도 수가 적은 단어에 대해서 임베딩의 정확도가 높지 않은 단점
- FastText는 희귀 단어라도 n-gram이 다른 단어의 n-gram과 겹치는 경우라면,   
  Word2Vec보다 비교적 높은 임베딩 벡터값을 얻음
- 오타와 같은 노이즈가 많은 코퍼스에서도 일정 수준의 성능을 보임 (apple, appple)

```python
from gensim.models import FastText

model = FastText(result, size=100, window=5, min_count=5, workers=4, sg=1)
```

## 09-08. Pre-trained Word Embedding
- 사전 훈련된 워드 임베딩 [참고](https://wikidocs.net/33793)

## 09-09. ELMo
- 언어 모델로 하는 임베딩이라는 뜻으로, 사전 훈련된 언어 모델을 사용
- Word2Vec는 Bank Account와 River Bank에서 Bank의 차이를 구분하지 못하지만,   
  ELMo는 문맥을 반영한 워드 임베딩을 수행
- ELMo 표현을 기존 임베딩 벡터와 연결(concatenate)해서 입력으로 사용 가능

### biLM
- RNN 언어 모델에서 $h_t$는 시점이 지날수록 업데이트되기 때문에,   
  문장의 문맥 정보를 점차적으로 반영함
- ELMo는 양쪽 방향의 언어 모델(biLM)을 학습하여 활용
- biLM은 은닉층이 최소 2개 이상인 다층 구조를 전제로 함
- 양방향 RNN은 순방향 RNN의 hidden state와 역방향 RNN의 hidden state를 연결하는 것이지만,   
  biLM은 순방향 언어 모델과 역방향 언어 모델이라는 두 개의 언어 모델을 별개의 모델로 보고 학습
- 각 층(embedding, hidden state)의 출력값이 가진 정보가 서로 다른 것이므로,   
  이를 모두 활용하여 순방향 언어 모델과 역방향 언어 모델의 각 층의 출력값을 연결

### ELMo Representation
1. 각 층의 출력값을 연결(concatenate)
2. 각 층의 출력값 별로 가중치($s_1, s_2, s_3$) 부여
3. 각 층의 출력값을 모두 더함 (2번과 3번을 요약하여 가중합이라 표현)
4. 벡터의 크기를 결정하는 스칼라 매개변수($\gamma$)를 곱함

### [ELMo 활용](https://wikidocs.net/33930)
- [스팸 메일 분류하기 데이터](https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv) 사용

```python
# 텐서플로우 1버전에서 사용 가능
%tensorflow_version 1.x
pip install tensorflow-hub
import tensorflow_hub as hub
```

```python
# 텐서플로우 허브로부터 ELMo를 다운로드
elmo = hub.Module("https://tfhub.dev/google/elmo/1", trainable=True)

sess = tf.Session()
K.set_session(sess)
sess.run(tf.global_variables_initializer())
sess.run(tf.tables_initializer())
```

```python
# 데이터의 이동이 케라스 → 텐서플로우 → 케라스가 되도록 하는 함수
def ELMoEmbedding(x):
    return elmo(tf.squeeze(tf.cast(x, tf.string)), as_dict=True, signature="default")
```

```python
from keras.models import Model
from keras.layers import Dense, Lambda, Input

input_text = Input(shape=(1,), dtype=tf.string)
embedding_layer = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text)
hidden_layer = Dense(256, activation='relu')(embedding_layer)
output_layer = Dense(1, activation='sigmoid')(hidden_layer)
model = Model(inputs=[input_text], outputs=output_layer)
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
```

## 09-10. Embedding Visualization
- 구글 [embedding projector](https://projector.tensorflow.org/) 시각화 도구 ([논문 참고](https://arxiv.org/pdf/1611.05469v1.pdf))

```python
# !python -m gensim.scripts.word2vec2tensor --input 모델이름 --output 모델이름
!python -m gensim.scripts.word2vec2tensor --input eng_w2v --output eng_w2v
# 임베딩 프로젝트에 사용할 metadata.tsv와 tensor.tsv 파일 생성
```

## 09-11. Document Embedding
- 문서 벡터를 이용한 추천 시스템 [참고](https://wikidocs.net/102705)
- 문서 임베딩 : 워드 임베딩의 평균 [참고](https://wikidocs.net/103496)
- Doc2Vec으로 공시 사업보고서 유사도 계산하기 [참고](https://wikidocs.net/155356)

---

## 10. RNN Text Classification
- [케라스를 이용한 텍스트 분류 개요](https://wikidocs.net/24873)
- [스팸 메일 분류하기](https://wikidocs.net/22894)
- [로이터 뉴스 분류하기](https://wikidocs.net/22933)
- [IMDB 리뷰 감성 분류하기](https://wikidocs.net/24586)
- [나이브 베이즈 분류기](https://wikidocs.net/22892)
- [네이버 영화 리뷰 감성 분류하기](https://wikidocs.net/44249)
- [네이버 쇼핑 리뷰 감성 분류하기](https://wikidocs.net/94600)
- [BiLSTM으로 한국어 스팀 리뷰 감성 분류하기](https://wikidocs.net/94748)

---




