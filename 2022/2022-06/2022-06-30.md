# NLP Study

---

## 15-01. Attention Mechanism
- seq2seq 모델은 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 해서 정보 손실이 발생하며,   
  RNN의 고질적인 문제인 기울기 소실 문제도 존재
  기계 번역 분야에서 입력 문장이 길면 번역 품질이 떨어지는 것을 보정하기 위해 어텐션 기법 활용
- 어텐션의 기본 아이디어는 디코더에서 출력 단어를 예측하는 매 시접마다,   
  인코더에서 전체 입력 문장을 다시 한 번 참고한다는 점

### Attention Function
- Attention(Q, K, V) = Attention Value
- 어텐션 함수는 주어진 Query에 대해서 모든 Key와의 유사도를 각각 구하고,   
  유사도를 키와 맵핑되어있는 각각의 Value에 반영, 이후 유사도가 반영된 Value를 모두 더해 리턴

> **Q(Query)**: t 시점의 디코더 셀에서의 은닉 상태   
> **K(Keys)**: 모든 시점의 인코더 셀의 은닉 상태들   
> **V(Values)**: 모든 시점의 인코더 셀의 은닉 상태들

### Dot-Product Attention
- 어텐션 메커니즘에서 출력 단어를 예측하기 위해 디코더 셀은   
  t-1의 hidden state, t-1에 나온 출력 단어, Attention Value $a_t를 필요
- 제안자의 이름을 따서 루옹(Luong) 어텐션이라고도 함

#### 1. Attention Score
- $a_t$를 구하기 위해서는 Attention Score를 구해야 함   
  (인코더의 모든 은닉 상태 각각의 디코더의 현 시점의 은닉 상태 $s_t$와 얼마나 유사한지 판단하는 스코어)
- Dot-Product Attention에서는 스코어 값을 구하기 위해 $s_t$를 전치하고 각 은닉 상태와 내적을 수행
- 스코어 함수 $score(s_t,h_i)={s^T_t}{h_i}$
- $s_t$와 인코더의 모든 은닉 상태의 어텐션 스코어 모음값 $e^t=[{s^T_t}{h_1},...,{s^T_t}{h_N}]$
- 스코어 함수에 따라 scaled dot, general, concat, location-base 어텐션 등이 존재

#### 2. Attention Distribution
- $e^t$에 softmax 함수를 적용하여 모든 값을 합하면 1이 되는 확률 분포,   
  Attention Distribution을 얻으며, 분포 각각의 값을 Attention Weight라 함
- 어텐션 분포 $\alpha^t=softmax(e^t)$

#### 3. Attention Value
- 어텐션의 최종 결과값을 얻기 위해서 각 인코더의 은닉 상태와 어텐션 가중치들을 곱하고,   
  최종적으로 모두 더하는 Weighted Sum을 진행
- 어텐션 값이 구해지면 어텐션 메커니즘은 $a_t$를 $s_t$와 결합(concatenate)하여 하나의 벡터 $v_t$를 생성
- $v_t$를 $\hat{y}$ 예측 연산의 입력으로 사용해 인코더로부터 얻은 정보를 활용하여 $\hat{y}$를 예측

$$a_t=\Sigma^N_{i=1}{\alpha^t_i}{h_i}$$

## 15-02. Bahdanau Attention
- 바다나우 어텐션 함수의 Query는 t 시점의 은닉 상태가 아니라 t-1 시점의 은닉 상태를 사용
- $score(s_{t-1},h_i)={W^T_\alpha}\tanh{({W_b}{s_{t-1}}+{W_c}{h_i})}$
- $W_a,W_b,W_c$는 학습 가능한 가중치 행렬을 의미하며, $s_{t-1}$와 $h_1,h_2,h_3,h_4$의 어텐션 스코어를   
  각각 구하는 병렬 연산을 위해 $h_1,h_2,h_3,h_4$를 하나의 행렬 $H$로 변환
- $e^t={W^T_\alpha}\tanh{({W_b}{s_{t-1}}+{W_c}H)}$
- 컨텍스트 벡터를 구하면, 현재 시점의 입력인 단어의 임베딩 벡터와 연결(concatenate)하고,   
  현재 시점의 새로운 입력으로 사용

---

## 16-01. Transformer
- 어텐션을 RNN의 보정을 위한 용도로 사용하지 않고, 어텐션만으로 인코더와 디코더를 생성

### Transformer Hyperparameter
$d_{model}=152$
> 트랜스 포머의 인코더와 디코더에서의 정해진 입력과 출력의 크기, 임베딩 벡터의 차원

$num\_layers=6$
> 트랜스포머 모델에서 인코더와 디코더가 총 몇 층으로 구성되었는지

$num\_heads=8$
> 어텐션을 여러 개로 분할해서 병렬로 수행하고 결과값을 다시 하나로 합치는데, 이때 이 병렬의 개수

$d_{ff}=2048$
> 피드 포워드 신경망의 은닉층의 크기, 입력층과 출력층의 크기는 $d_{model}$

### Positional Encoding
- RNN은 단어의 위치에 따라 단어를 순차적으로 입력받아 처리하는 특성으로 인해 각 단어의 위치 정보를 가짐
- 트랜스포머는 단어 입력을 순차적으로 받는 방식이 아니기 때문에   
  단어의 위치 정보를 얻기 위해 각 단어의 임베딩 벡터에 위치 정보들을 더해 모델의 입력으로 사용(포지셔널 인코딩)
- 트랜스포머는 위치 정보를 가진 값을 만들기 위해 아래 함수를 사용

$$PE_{(pos,2i)}=\sin{(pos/10000^{2i/d_{model}})}$$
$$PE_{(pos,2i+1)}=\cos{(pos/10000^{2i/d_{model}})}$$

- 사인 함수와 코사인 함수의 값을 임베딩 벡터에 더해 단어에 순서 정보를 부여
- $pos$는 입력 문장에서의 임베딩 벡터의 위치, $i$는 임데빙 벡터 내의 차원의 인덱스 의미
- $d_{model}$은 트랜스포머의 모든 층의 출력 차원을 의미하는 트랜스포머의 하이퍼파라미터

### Self-Attention
- 어텐션을 자기 자신에게 수행하는 것
- Q, K, V는 모두 입력 문장의 모든 단어 벡터들을 의미
- 셀프 어텐션은 입력 문장 내의 단어들끼리 유사도를 구해 it이 어떤 단어와 연관되었는지 확률을 찾아냄
- 셀프 어텐션은 인코더의 초기 입력인 $d_{model}$의 차원을 가지는 단어 벡터들을 사용하여 수행하지 않고,   
  각 단어 벡터들로부터 Q벡터, K벡터, V벡터를 얻는 작업을 거침
- $d_{model}=512$의 차원을 가졌던 각 단어 벡터들은 Q벡터, K벡터, V벡터로 변환되면서   
  $d_{model}$을 $num\_heads$로 나눈 값 64를 차원으로 갖게 됨

### Scaled dot-product Attention
- 트랜스포머에서는 스케일드 닷-프로적트 어텐션을 사용
- 벡터마다 일일히 컨텍스트 벡터를 구하는 벡터 연산을 하지 않고,   
  문장 행렬에 가중치 행렬을 곱하여 구한 Q행렬, K행렬, V행렬에 행렬 연산을 수행
- 행렬 연산에서 어텐션 스코어는 행렬의 값에 전체적으로 $\sqrt{d_k}$를 나누어 스코어 값을 가지는 행렬을 구함

$$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$$

### Multi-head Attention
- 한 번의 어텐션을 하는 것보다 여러번의 어텐션을 병렬로 사용하는 것이 더 효과적이기 때문에   
  $d_{model}$의 차원을 $num\_heads$개로 나누어 Q, K, V에 대해서 $num\_heads$개의 병렬 어텐션을 수행
- 각각의 어텐션 값 행렬을 어텐션 헤드라고 불는데, 이때 가중치 행렬 $W^Q, W^K, W^V$의 값은 어텐션 헤드마다 전부 다름
- 멀티 헤드 어텐션은 어텐션을 병렬로 수행하여 다른 시각으로 정보들을 수집
- 벙렬 어텐션 수행 후 모든 어텐션 헤드를 연결(concatenate)하여 $(seq\_len, d_{model})$ 크기의 행렬 생성
- 연결한 행렬에 가중치 행렬 $W^O$를 곱한 것이 멀티 헤드 어텐션의 최종 결과물이며, 이는 인코더의 입력이었던 문장 행렬과 동일   
  (인코더에서의 입력의 킉가 출력에서도 동일 크기로 계쏙 유지되어야만 다음 인코더에서도 다시 입력이 될 수 있음)

### Padding Mask
- 입력 문장에 <PAD> 토큰이 있을 경우 어텐션에서 제외하기 위해 -1e9라는 아주 작은 음수 값을 곱함
- **Masking**: 어텐션에서 제외하기 위해 값을 가리는 것
- 어텐션 스코어 행렬에서 행에 해당하는 문장은 Query, 열에 해당하는 문장은 Key이며,   
  Key에 <PAD>가 있는 경우 해당 열 전체를 마스킹

### Position-wise FFNN
- 포지션 와이즈 FFNN은 인코더와 디코더에서 공통적으로 가지고 있는 서브층으로, 완전 연결 FFNN을 의미
- $FFNN(x)=MAX(0,x{W_1}+b_1){W_2}+{b_2}$
- $x$는 멀티 헤드 어텐션의 결과로 나온 $(seq\_len, d_{model})$ 크기의 행렬을 의미,   
  가중치 행렬 $W_1$은 $(d_{model},d_{ff})$, $W_2$은 $(d_{ff},d_{model})$의 크기를 가짐
- 서브층을 지난 인코더의 최종 출력은 여전히 인코더의 입력의 크기였던 $(seq\_len, d_{model})$의 크기가 보존

### Residual Connection
- 트랜스포머에서 두 개의 서브층을 가진 인코더에 추가적으로 사용하는 Add & Norm 기법 중 Add에 해당
- 잔차 연결은 서브층의 입력과 출력을 더하는 것으로, 컴퓨터 비전 분야에서 주로 사용되는 모델의 학습을 돕는 기법
- $x+Sublayer(x)$로 표현할 수 있으며, 멀티 헤드 어텐션이라면 $H(x)=x+Multi\text{-}head\ Attention(x)$과 같음

### Layer Normalization
- 잔차 연결의 입력을 $x$, 잔차 연결과 층 정규화 두 가지 연산을 모두 수행한 후의 결과 행렬을 $LN$이라 할 때,   
  잔차 연결 후 정규화 연산을 수식으로 표현하면 $LN=LayerNorm(x+Sublayer(x))$와 같음
- 총 정규화는 텐서의 마지막 차원에 대해서 평균과 분산을 구하고, 이를 가지고 어떤 수식을 통해 값을 정규화하여 학습을 도움
- 총 정규화를 평균과 분산을 통한 정규화, 감마와 베타를 도입하는 것으로 나누었을 때,   
  우선, 평균과 분산을 통해 벡터 $x_i$를 정규화
- $x_i$는 벡터인 반면, 평균 $\mu_i$과 분산 $\sigma^2_i$은 스칼라이기 때문에,   
  벡터 $x_i$의 각 $k$차원의 값은 다음과 같이 정규화 ($\epsilon$은 분모가 0이 되는 것을 방지하는 값)

$$\hat{x}_{i,k}=\frac{x_{i,k}-\mu_i}{\sqrt{\sigma^2_i+\epsilon}}$$

### Look-ahead Mask
- 입력 단어를 매 시점마다 순차적으로 입력받는 RNN 계열의 신경망에 반해,   
  트랜스포머는 문장 행렬을 한 번에 받기 때문에 미래 시점의 단어까지 참고할 수 있는 현상 발생
- 룩-어헤드 마스크는 디코더의 첫번째 서브층에서 이루어지며,   
  자기 자신보다 미래에 있는 단어들은 참고하지 못하도록 마스킹함
- 룩-어헤드 마스크를 한다고해서 패딩 마스크가 불필요한 것이 아니므로 패딩 마스크를 포함하도록 구현

### Endocer-Decoder Attention
- 디코더의 두번째 서브층은 멀티 헤드 어텐션을 수행한다는 점에서 이전의 어텐션들과 공통점이 있지만,   
  Query와 Key, Value가 달라 셀프 어텐션이 아님

> 인코더의 첫번째 서브층 Query = Key = Value   
> 디코더의 첫번째 서브층 Query = Key = Value   
> 디코더의 두번째 서브층 Query: 디코더 행렬 / Key = Value: 인코더 행렬

## 16-02. Transformer Chatbot
- 트랜스포머를 이용한 한국어 챗봇 [참고](https://wikidocs.net/89786)
- [챗본 데이터](https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv) 사용

---

## 17-01. Pre-training

### 사전 훈련된 워드 임베딩
- Word2Vec나 GloVe 등의 워드 임베딩은 하나의 단어가 하나의 벡터값으로 맵핑되므로,   
  문맥을 고려하지 못하여 다의어나 동음이의어를 구분하지 못하는 문제

### 사전 훈련된 언어 모델
- 언어 모델은 주어진 텍스트로부터 다음 단어를 예측하도록 학습하여 별도의 라벨 없이 학습 가능
- 다의어를 구분할 수 없었던 문제점을 해결하고, RNN 계열의 신경망에서 탈피하기 위해 트랜스포머로 학습 시도
- 트랜스포머로 데이터를 학습시킨 언어 모델 GPT-1 등은 다양한 태스크에서 높은 성능을 얻음
- 이전 단어들로부터 다음 단어를 예측하는 언어 모델의 특성으로는 양방향 언어 모델을 사용할 수 없으므로,   
  ELMo에서는 두 개의 단방향 언어 모델을 따로 준비하여 학습하는 방법을 사용했는데 여기서 발전된 마스크드 언어 모델이 등장

### Masked Language Model
- 마스크드 언어 모델은 입력 텍스트의 단어 집합의 15%의 단어를 랜덤으로 Masking
- 빈칸 채우기 형식으로 마스킹된 단어들을 예측하게 함

## 17-02. BERT
- 구글이 공개한 사전 훈련된 모델로 수많은 NLP 태스크에서 최고 성능을 보임
- 트랜스포머를 이용하여 구현되었으며, 위키피디아와 BooksCorpus와 같은 라벨이 없는 텍스트 데이터로 사전 훈련된 언어 모델
- 사전 훈련된 모델을 라벨이 있는 다른 작업에서 추가 훈련과 함께 하이퍼파라미터를 재조정,   
  다른 작업에 대해서 파라미터 재조정을 위한 추가 훈련 과정을 Fine-tuning이라고 함
- BERT-Base: L=12, D=768, A=12: 110M개의 파라미터
- BERT-Large: L=24, D=1024, A=16: 340M개의 파라미터

### Contextual Embedding
- BERT의 입력은 기존 모델들과 마찬가지로 임베딩 층을 지난 임베딩 벡터
- BERT의 연산을 거친 후의 출력 임베딩은 문장의 문맥을 모두 참고한 문맥을 반영한 임베딩이 됨
- BERT는 트랜스포머 인코더를 12번 쌓은 구조로, 셀프 어텐션을 통해 문맥을 반영

### Subword Tokenizer
- BERT는 서브워드 토크나이저로 WordPiece 토크나이저를 사용
- 자주 등장하는 단어는 그대로 단어 집합에 추가하지만, 자주 등장하지 않는 단어는 서브워드로 분리해 집합에 추가하며,   
  집합이 만들어지고 나면 단어 집합을 기반으로 토큰화 수행
- BERT에서 토큰이 단어 집합에 존재할 경우 해당 토큰을 분리하지 않지만,   
  존재하지 않으면 토큰을 서브워드로 분리하고, 첫번째 서브워드를 제외한 나머지 서브워드들은 앞에 ##를 붙인 것을 토큰으로 함
- ##은 서브워드들이 단어의 중간부터 등장하는 것을 알려주기 위해 표시해둔 기호

```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased") # BERT-Base의 토크나이저
```

### Position Embedding
- 포지셔널 인코딩과 유사하지만, 위치 정보를 사인 함수와 코사인 함수가 아닌 학습을 통해서 얻는 방법
- 위치 정보를 위한 임베딩 층을 하나 더 사용하고, 입력마다 포지션 임베딩 벡터를 더해줌

### MLM (Pre-training)
- BERT는 사전 훈련을 위해서 입력 텍스트의 15%의 단어를 랜덤으로 마스킹
- [MASK] 토큰이 파인 튜닝 단계에서 나타나지 않아 사전 학습 단계와 파인 튜닝 단계에서의 불일치 문제가 생기는데,   
  이를 완화하기 위해 마스킹 단어 중 80%는 [MASK]로 변경, 10%는 랜덤으로 단어가 변경, 10%는 동일하게 둠

### NSP (Pre-training)
- BERT는 두 개의 문장을 준 후에 이어지는 문장인지 아닌지를 맞추는 방식으로 훈련
- BERT의 입력에서 [SEP]라는 특별 토큰을 사용해서 문장을 구분
- 두 문장이 실제 이어지는 문장인지 아닌지에 대한 이진 분류 문제를 [CLS] 토큰의 위치로 결정

### Segment Embedding
- WordPiece Embedding, Position Embedding과 함께 두 개의 문장을 구분하기 위한 목적으로 사용되는 임베딩 층
- 세그먼트 임베딩으로 구분되는 BERT의 입력에서 두 개의 문장은 두 종류의 텍스트, 두 개의 문서일 수 있음

### Find-tuning
- 영화 리뷰 감성 분류, 로이터 뉴스 분류 등 **Single Text Classification**을 위해,   
  문서의 시작에 [CLS] 토큰을 입력해 분류에 대해 예측
- **태깅 작업**을 위해 각 토큰의 위치에 밀집층을 사용하여 분류에 대해 예측
- 자연어 추론 등의 **Text Pair Classification**을 위해,   
  텍스트 사이에 [SEP] 토큰을 집어넣고 두 종류의 세그먼트 임베딩을 모두 사용하여 문서를 구분
- QA(Question Answering)를 풀기 위해 질문과 본문이라는 두 개의 텍스트의 쌍을 입력 (SQuAD v1.1)

### Attention Mask
- BERT가 어텐션 연산을 할 때, 불필요하게 패딩 토큰에 대해서 어텐션을 하지 않도록 실제 단어와 패딩 토큰을 구분하는 입력
- 숫자 1은 실제 단어로 마스킹을 하지 않고, 숫자 0은 패딩 토큰으로 마스킹을 함

## 17-03. Pre-training 실습
- 구글 BERT의 마스크드 언어 모델 실습 [참고](https://wikidocs.net/153992)
- 한국어 BERT의 마스크드 언어 모델 실습 [참고](https://wikidocs.net/152922)
- 구글 BERT의 다음 문장 예측 [참고](https://wikidocs.net/156767)
- 한국어 BERT의 다음 문장 예측 [참고](https://wikidocs.net/156774)

## 17-07. Sentence BERT(SBERT)
- BERT의 문장 임베딩의 성능을 우수하게 개선시킨 모델
- 문장 쌍 분류 태스크 또는 문장 쌍 회귀 태스크로 파인 튜닝

### Sentence Embedding
- [CLS] 토큰은 입력된 문장에 대한 총체적 표현으로, [CLS] 토큰 자체를 입력 문장의 벡터로 간주
- 문장 벡터를 얻기 위해 [CLS] 토큰뿐 아니라, BERT의 모든 출력 벡터들을 평균냄
- 출력 벡터들의 평균을 pooling이라 하며, mean pooling, max pooling 등이 있음

---

## 18. BERT 실습










