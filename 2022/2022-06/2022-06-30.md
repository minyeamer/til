---
layout: post
title: 2022-06-30 Log
date: 2022-06-30 20:00:00 +0900
summary: 딥 러닝을 이용한 자연어 처리 입문 3
categories: [Study, "2022", "2022-06"]
tags: [TIL, NLP]
cover:
  image: calendar.jpg
---

# 15-01. Attention Mechanism
- seq2seq 모델은 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 해서 정보 손실이 발생하며,   
  RNN의 고질적인 문제인 기울기 소실 문제도 존재
  기계 번역 분야에서 입력 문장이 길면 번역 품질이 떨어지는 것을 보정하기 위해 어텐션 기법 활용
- 어텐션의 기본 아이디어는 디코더에서 출력 단어를 예측하는 매 시접마다,   
  인코더에서 전체 입력 문장을 다시 한 번 참고한다는 점

## Attention Function
- Attention(Q, K, V) = Attention Value
- 어텐션 함수는 주어진 Query에 대해서 모든 Key와의 유사도를 각각 구하고,   
  유사도를 키와 맵핑되어있는 각각의 Value에 반영, 이후 유사도가 반영된 Value를 모두 더해 리턴

> **Q(Query)**: t 시점의 디코더 셀에서의 은닉 상태   
> **K(Keys)**: 모든 시점의 인코더 셀의 은닉 상태들   
> **V(Values)**: 모든 시점의 인코더 셀의 은닉 상태들

## Dot-Product Attention
- 어텐션 메커니즘에서 출력 단어를 예측하기 위해 디코더 셀은   
  t-1의 hidden state, t-1에 나온 출력 단어, Attention Value $a_t$를 필요
- 제안자의 이름을 따서 루옹(Luong) 어텐션이라고도 함

## 1. Attention Score
- $a_t$를 구하기 위해서는 Attention Score를 구해야 함   
  (인코더의 모든 은닉 상태 각각의 디코더의 현 시점의 은닉 상태 $s_t$와 얼마나 유사한지 판단하는 스코어)
- Dot-Product Attention에서는 스코어 값을 구하기 위해 $s_t$를 전치하고 각 은닉 상태와 내적을 수행
- 스코어 함수 $score(s_t,h_i)={s^T_t}{h_i}$
- $s_t$와 인코더의 모든 은닉 상태의 어텐션 스코어 모음값 $e^t=[{s^T_t}{h_1},...,{s^T_t}{h_N}]$
- 스코어 함수에 따라 scaled dot, general, concat, location-base 어텐션 등이 존재

## 2. Attention Distribution
- $e^t$에 softmax 함수를 적용하여 모든 값을 합하면 1이 되는 확률 분포,   
  Attention Distribution을 얻으며, 분포 각각의 값을 Attention Weight라 함
- 어텐션 분포 $\alpha^t=softmax(e^t)$

## 3. Attention Value
- 어텐션의 최종 결과값을 얻기 위해서 각 인코더의 은닉 상태와 어텐션 가중치들을 곱하고,   
  최종적으로 모두 더하는 Weighted Sum을 진행
- 어텐션 값이 구해지면 어텐션 메커니즘은 $a_t$를 $s_t$와 결합(concatenate)하여 하나의 벡터 $v_t$를 생성
- $v_t\text{를 }\hat{y}$ 예측 연산의 입력으로 사용해 인코더로부터 얻은 정보를 활용하여 $\hat{y}$를 예측

$$a_t=\Sigma^N_{i=1}{\alpha^t_i}{h_i}$$

# 15-02. Bahdanau Attention
- 바다나우 어텐션 함수의 Query는 t 시점의 은닉 상태가 아니라 t-1 시점의 은닉 상태를 사용
- $score(s_{t-1},h_i)={W^T_\alpha}\tanh{({W_b}{s_{t-1}}+{W_c}{h_i})}$
- $W_a,W_b,W_c$는 학습 가능한 가중치 행렬을 의미하며, $s_{t-1}$와 $h_1,h_2,h_3,h_4$의 어텐션 스코어를   
  각각 구하는 병렬 연산을 위해 $h_1,h_2,h_3,h_4$를 하나의 행렬 $H$로 변환
- $e^t={W^T_\alpha}\tanh{({W_b}{s_{t-1}}+{W_c}H)}$
- 컨텍스트 벡터를 구하면, 현재 시점의 입력인 단어의 임베딩 벡터와 연결(concatenate)하고,   
  현재 시점의 새로운 입력으로 사용

---

# 16-01. Transformer
- 어텐션을 RNN의 보정을 위한 용도로 사용하지 않고, 어텐션만으로 인코더와 디코더를 생성

## Transformer Hyperparameter
$d_{model}=152$
> 트랜스 포머의 인코더와 디코더에서의 정해진 입력과 출력의 크기, 임베딩 벡터의 차원

$num_{layers}=6$
> 트랜스포머 모델에서 인코더와 디코더가 총 몇 층으로 구성되었는지

$num_{heads}=8$
> 어텐션을 여러 개로 분할해서 병렬로 수행하고 결과값을 다시 하나로 합치는데, 이때 이 병렬의 개수

$d_{ff}=2048$
> 피드 포워드 신경망의 은닉층의 크기, 입력층과 출력층의 크기는 $d_{model}$

## Positional Encoding
- RNN은 단어의 위치에 따라 단어를 순차적으로 입력받아 처리하는 특성으로 인해 각 단어의 위치 정보를 가짐
- 트랜스포머는 단어 입력을 순차적으로 받는 방식이 아니기 때문에   
  단어의 위치 정보를 얻기 위해 각 단어의 임베딩 벡터에 위치 정보들을 더해 모델의 입력으로 사용(포지셔널 인코딩)
- 트랜스포머는 위치 정보를 가진 값을 만들기 위해 아래 함수를 사용

$$PE_{(pos,2i)}=\sin{(pos/10000^{2i/d_{model}})}$$
$$PE_{(pos,2i+1)}=\cos{(pos/10000^{2i/d_{model}})}$$

- 사인 함수와 코사인 함수의 값을 임베딩 벡터에 더해 단어에 순서 정보를 부여
- $pos$는 입력 문장에서의 임베딩 벡터의 위치, $i$는 임데빙 벡터 내의 차원의 인덱스 의미
- $d_{model}$은 트랜스포머의 모든 층의 출력 차원을 의미하는 트랜스포머의 하이퍼파라미터

## Self-Attention
- 어텐션을 자기 자신에게 수행하는 것
- Q, K, V는 모두 입력 문장의 모든 단어 벡터들을 의미
- 셀프 어텐션은 입력 문장 내의 단어들끼리 유사도를 구해 it이 어떤 단어와 연관되었는지 확률을 찾아냄
- 셀프 어텐션은 인코더의 초기 입력인 $d_{model}$의 차원을 가지는 단어 벡터들을 사용하여 수행하지 않고,   
  각 단어 벡터들로부터 Q벡터, K벡터, V벡터를 얻는 작업을 거침
- $d_{model}=512$의 차원을 가졌던 각 단어 벡터들은 Q벡터, K벡터, V벡터로 변환되면서   
  $d_{model}$을 $num_{heads}$로 나눈 값 64를 차원으로 갖게 됨

## Scaled dot-product Attention
- 트랜스포머에서는 스케일드 닷-프로적트 어텐션을 사용
- 벡터마다 일일히 컨텍스트 벡터를 구하는 벡터 연산을 하지 않고,   
  문장 행렬에 가중치 행렬을 곱하여 구한 Q행렬, K행렬, V행렬에 행렬 연산을 수행
- 행렬 연산에서 어텐션 스코어는 행렬의 값에 전체적으로 $\sqrt{d_k}$를 나누어 스코어 값을 가지는 행렬을 구함

$$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$$

## Multi-head Attention
- 한 번의 어텐션을 하는 것보다 여러번의 어텐션을 병렬로 사용하는 것이 더 효과적이기 때문에   
  $d_{model}$의 차원을 $num_{heads}$개로 나누어 Q, K, V에 대해서 $num_{heads}$개의 병렬 어텐션을 수행
- 각각의 어텐션 값 행렬을 어텐션 헤드라고 불는데, 이때 가중치 행렬 $W^Q, W^K, W^V$의 값은 어텐션 헤드마다 전부 다름
- 멀티 헤드 어텐션은 어텐션을 병렬로 수행하여 다른 시각으로 정보들을 수집
- 벙렬 어텐션 수행 후 모든 어텐션 헤드를 연결(concatenate)하여 $(seq_{len}, d_{model})$ 크기의 행렬 생성
- 연결한 행렬에 가중치 행렬 $W^O$를 곱한 것이 멀티 헤드 어텐션의 최종 결과물이며, 인코더의 입력이었던 문장 행렬과 동일   
  (인코더에서의 입력의 킉가 출력에서도 동일 크기로 계쏙 유지되어야만 다음 인코더에서도 다시 입력이 될 수 있음)

## Padding Mask
- 입력 문장에 <PAD> 토큰이 있을 경우 어텐션에서 제외하기 위해 -1e9라는 아주 작은 음수 값을 곱함
- **Masking**: 어텐션에서 제외하기 위해 값을 가리는 것
- 어텐션 스코어 행렬에서 행에 해당하는 문장은 Query, 열에 해당하는 문장은 Key이며,   
  Key에 <PAD>가 있는 경우 해당 열 전체를 마스킹

## `Position-wise` FFNN
- 포지션 와이즈 FFNN은 인코더와 디코더에서 공통적으로 가지고 있는 서브층으로, 완전 연결 FFNN을 의미
- $FFNN(x)=MAX(0,x{W_1}+b_1){W_2}+{b_2}$
- $x$는 멀티 헤드 어텐션의 결과로 나온 $(seq_{len}, d_{model})$ 크기의 행렬을 의미,   
  가중치 행렬 $W_1\text{은 }(d_{model},d_{ff})\text{, }W_2\text{은 }(d_{ff},d_{model})$의 크기를 가짐
- 서브층을 지난 인코더의 최종 출력은 여전히 인코더의 입력의 크기였던 $(seq_{len}, d_{model})$의 크기가 보존

## Residual Connection
- 트랜스포머에서 두 개의 서브층을 가진 인코더에 추가적으로 사용하는 Add & Norm 기법 중 Add에 해당
- 잔차 연결은 서브층의 입력과 출력을 더하는 것으로, 컴퓨터 비전 분야에서 주로 사용되는 모델의 학습을 돕는 기법
- $x+Sublayer(x)$로 표현할 수 있으며, 멀티 헤드 어텐션이라면 $H(x)=x+Multi\text{-}head\ Attention(x)$과 같음

## Layer Normalization
- 잔차 연결의 입력을 $x$, 잔차 연결과 층 정규화 두 가지 연산을 모두 수행한 후의 결과 행렬을 $LN$이라 할 때,   
  잔차 연결 후 정규화 연산을 수식으로 표현하면 $LN=LayerNorm(x+Sublayer(x))$와 같음
- 총 정규화는 텐서의 마지막 차원에 대해서 평균과 분산을 구하고, 이를 가지고 어떤 수식을 통해 값을 정규화하여 학습을 도움
- 총 정규화를 평균과 분산을 통한 정규화, 감마와 베타를 도입하는 것으로 나누었을 때,   
  우선, 평균과 분산을 통해 벡터 $x_i$를 정규화
- $x_i$는 벡터인 반면, 평균 $\mu_i\text{과 분산 }\sigma^2_i$은 스칼라이기 때문에,   
  벡터 $x_i$의 각 $k$차원의 값은 다음과 같이 정규화 ($\epsilon$은 분모가 0이 되는 것을 방지하는 값)

$$\hat{x_{i,k}}=\frac{x_{i,k}-\mu_i}{\sqrt{\sigma^2_i+\epsilon}}$$

## Look-ahead Mask
- 입력 단어를 매 시점마다 순차적으로 입력받는 RNN 계열의 신경망에 반해,   
  트랜스포머는 문장 행렬을 한 번에 받기 때문에 미래 시점의 단어까지 참고할 수 있는 현상 발생
- 룩-어헤드 마스크는 디코더의 첫번째 서브층에서 이루어지며,   
  자기 자신보다 미래에 있는 단어들은 참고하지 못하도록 마스킹함
- 룩-어헤드 마스크를 한다고해서 패딩 마스크가 불필요한 것이 아니므로 패딩 마스크를 포함하도록 구현

## Endocer-Decoder Attention
- 디코더의 두번째 서브층은 멀티 헤드 어텐션을 수행한다는 점에서 이전의 어텐션들과 공통점이 있지만,   
  Query와 Key, Value가 달라 셀프 어텐션이 아님

> 인코더의 첫번째 서브층 Query = Key = Value   
> 디코더의 첫번째 서브층 Query = Key = Value   
> 디코더의 두번째 서브층 Query: 디코더 행렬 / Key = Value: 인코더 행렬

# 16-02. Transformer Chatbot
- 트랜스포머를 이용한 한국어 챗봇 [참고](https://wikidocs.net/89786)
- [챗본 데이터](https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv) 사용

---

# 17-01. Pre-training

## 사전 훈련된 워드 임베딩
- Word2Vec나 GloVe 등의 워드 임베딩은 하나의 단어가 하나의 벡터값으로 맵핑되므로,   
  문맥을 고려하지 못하여 다의어나 동음이의어를 구분하지 못하는 문제

## 사전 훈련된 언어 모델
- 언어 모델은 주어진 텍스트로부터 다음 단어를 예측하도록 학습하여 별도의 라벨 없이 학습 가능
- 다의어를 구분할 수 없었던 문제점을 해결하고, RNN 계열의 신경망에서 탈피하기 위해 트랜스포머로 학습 시도
- 트랜스포머로 데이터를 학습시킨 언어 모델 GPT-1 등은 다양한 태스크에서 높은 성능을 얻음
- 이전 단어들로부터 다음 단어를 예측하는 언어 모델의 특성으로는 양방향 언어 모델을 사용할 수 없으므로,   
  ELMo에서는 두 개의 단방향 언어 모델을 따로 준비해 학습하는 방법을 사용했는데 여기서 발전된 마스크드 언어 모델이 등장

## Masked Language Model
- 마스크드 언어 모델은 입력 텍스트의 단어 집합의 15%의 단어를 랜덤으로 Masking
- 빈칸 채우기 형식으로 마스킹된 단어들을 예측하게 함

# 17-02. BERT
- 구글이 공개한 사전 훈련된 모델로 수많은 NLP 태스크에서 최고 성능을 보임
- 트랜스포머를 이용하여 구현되었으며, 위키피디아와 BooksCorpus 같이 라벨이 없는 텍스트 데이터로 사전 훈련된 언어 모델
- 사전 훈련된 모델을 라벨이 있는 다른 작업에서 추가 훈련과 함께 하이퍼파라미터를 재조정,   
  다른 작업에 대해서 파라미터 재조정을 위한 추가 훈련 과정을 Fine-tuning이라고 함
- BERT-Base: L=12, D=768, A=12: 110M개의 파라미터
- BERT-Large: L=24, D=1024, A=16: 340M개의 파라미터

## Contextual Embedding
- BERT의 입력은 기존 모델들과 마찬가지로 임베딩 층을 지난 임베딩 벡터
- BERT의 연산을 거친 후의 출력 임베딩은 문장의 문맥을 모두 참고한 문맥을 반영한 임베딩이 됨
- BERT는 트랜스포머 인코더를 12번 쌓은 구조로, 셀프 어텐션을 통해 문맥을 반영

## Subword Tokenizer
- BERT는 서브워드 토크나이저로 WordPiece 토크나이저를 사용
- 자주 등장하는 단어는 그대로 단어 집합에 추가하지만, 자주 등장하지 않는 단어는 서브워드로 분리해 집합에 추가하며,   
  집합이 만들어지고 나면 단어 집합을 기반으로 토큰화 수행
- BERT에서 토큰이 단어 집합에 존재할 경우 해당 토큰을 분리하지 않지만,   
  존재하지 않으면 토큰을 서브워드로 분리하고, 첫번째 서브워드를 제외한 나머지 서브워드들은 앞에 #를 붙인 것을 토큰으로 함
- #은 서브워드들이 단어의 중간부터 등장하는 것을 알려주기 위해 표시해둔 기호

```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased") # BERT-Base의 토크나이저
```

## Position Embedding
- 포지셔널 인코딩과 유사하지만, 위치 정보를 사인 함수와 코사인 함수가 아닌 학습을 통해서 얻는 방법
- 위치 정보를 위한 임베딩 층을 하나 더 사용하고, 입력마다 포지션 임베딩 벡터를 더해줌

## MLM (Pre-training)
- BERT는 사전 훈련을 위해서 입력 텍스트의 15%의 단어를 랜덤으로 마스킹
- [MASK] 토큰이 파인 튜닝 단계에서 나타나지 않아 사전 학습 단계와 파인 튜닝 단계에서의 불일치 문제가 생기는데,   
  이를 완화하기 위해 마스킹 단어 중 80%는 [MASK]로 변경, 10%는 랜덤으로 단어가 변경, 10%는 동일하게 둠

## NSP (Pre-training)
- BERT는 두 개의 문장을 준 후에 이어지는 문장인지 아닌지를 맞추는 방식으로 훈련
- BERT의 입력에서 [SEP]라는 특별 토큰을 사용해서 문장을 구분
- 두 문장이 실제 이어지는 문장인지 아닌지에 대한 이진 분류 문제를 [CLS] 토큰의 위치로 결정

## Segment Embedding
- WordPiece Embedding, Position Embedding과 함께 두 개의 문장을 구분하기 위한 목적으로 사용되는 임베딩 층
- 세그먼트 임베딩으로 구분되는 BERT의 입력에서 두 개의 문장은 두 종류의 텍스트, 두 개의 문서일 수 있음

## Find-tuning
- 영화 리뷰 감성 분류, 로이터 뉴스 분류 등 **Single Text Classification**을 위해,   
  문서의 시작에 [CLS] 토큰을 입력해 분류에 대해 예측
- **태깅 작업**을 위해 각 토큰의 위치에 밀집층을 사용하여 분류에 대해 예측
- 자연어 추론 등의 **Text Pair Classification**을 위해,   
  텍스트 사이에 [SEP] 토큰을 집어넣고 두 종류의 세그먼트 임베딩을 모두 사용하여 문서를 구분
- QA(Question Answering)를 풀기 위해 질문과 본문이라는 두 개의 텍스트의 쌍을 입력 (SQuAD v1.1)

## Attention Mask
- BERT가 어텐션 연산을 할 때, 불필요하게 패딩 토큰에 대해서 어텐션을 하지 않도록 실제 단어와 패딩 토큰을 구분하는 입력
- 숫자 1은 실제 단어로 마스킹을 하지 않고, 숫자 0은 패딩 토큰으로 마스킹을 함

# 17-03. Pre-training 실습
- 구글 BERT의 마스크드 언어 모델 실습 [참고](https://wikidocs.net/153992)
- 한국어 BERT의 마스크드 언어 모델 실습 [참고](https://wikidocs.net/152922)
- 구글 BERT의 다음 문장 예측 [참고](https://wikidocs.net/156767)
- 한국어 BERT의 다음 문장 예측 [참고](https://wikidocs.net/156774)

# 17-07. Sentence BERT(SBERT)
- BERT의 문장 임베딩의 성능을 우수하게 개선시킨 모델
- 문장 쌍 분류 태스크 또는 문장 쌍 회귀 태스크로 파인 튜닝

## Sentence Embedding
- [CLS] 토큰은 입력된 문장에 대한 총체적 표현으로, [CLS] 토큰 자체를 입력 문장의 벡터로 간주
- 문장 벡터를 얻기 위해 [CLS] 토큰뿐 아니라, BERT의 모든 출력 벡터들을 평균냄
- 출력 벡터들의 평균을 pooling이라 하며, mean pooling, max pooling 등이 있음

---

# 18. BERT 실습
- [Colab에서 TPU 사용하기](https://wikidocs.net/119990)
- [Transformers의 모델 클래스 불러오기](https://wikidocs.net/158085)
- [KoBERT를 이용한 네이버 영화 리뷰 분류하기](https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/18.%20Fine-tuning%20BERT%20(Cls%2C%20NER%2C%20NLI)/18-3.%20kor_bert_nsmc_tpu.ipynb)
- [TFBertForSequenceClassification](https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/18.%20Fine-tuning%20BERT%20(Cls%2C%20NER%2C%20NLI)/18-4.%20kor_bert_nsmc_model_from_transformers_gpu.ipynb)
- [KoBERT를 이용한 KorNLI 풀어보기 (다중 클래스 분류)](https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/18.%20Fine-tuning%20BERT%20(Cls%2C%20NER%2C%20NLI)/18-5.%20kor_bert_kornli_model_from_transformers_tpu.ipynb)
- [KoBERT를 이용한 개체명 인식](https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/18.%20Fine-tuning%20BERT%20(Cls%2C%20NER%2C%20NLI)/18-6.%20kor_bert_ner_model_from_transformers_tpu.ipynb)
- [KoBERT를 이용한 기계 독해](https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/18.%20Fine-tuning%20BERT%20(Cls%2C%20NER%2C%20NLI)/18-7.%20kor_bert_question_answering_tpu.ipynb)
- [BERT의 문장 임베딩(SBERT)을 이용한 한국어 챗봇](https://wikidocs.net/154530)
- [Faiss와 SBERT를 이용한 시맨틱 검색기](https://wikidocs.net/162007)

---

# 19. Topic Modelling
- 토픽이라는 문서 집합의 추상적인 주제를 발견하기 위한 통계적 모델 중 하나
- 텍스트 본문의 숨겨진 의미 구조를 발견하기 위해 사용되는 텍스트 마이닝 기법

# 19-01. LSA

## SVD
- 특이값 분해(Singular Value Decomposition)는 $A$가 ${m}\times{m}$ 행렬일 때,   
  다음과 같이 3개의 행렬의 곱으로 분해(decomposition)하는 것

$$A={U}\Sigma{V^T}$$

- 각 3개 행렬은 다음과 같은 조건을 만족   
  ${U}\text{: }{m}\times{m}\text{ 직교행렬 }(AA^T=U(\Sigma\Sigma^T)U^T)$   
  $V\text{: }{n}\times{n}\text{ 직교행렬 }(A^TA=U(\Sigma^T\Sigma)V^T)$   
  $\Sigma\text{: }{m}\times{n}\text{ 직사각 대각행렬}$

## Truncated SVD
- Full SVD에서 나온 3개의 행렬에서 일부 벡터들을 삭제시킨 절단된 SVD
- 대각 행렬 $\Sigma$의 대각 원소의 값 중에서 상위값 t개만 남기고,   
  U행렬과 V행렬의 t열까지만 남김
- 일부 벡터들을 삭제해 데이터의 차원을 줄이는 것으로 계산 비용이 낮아지는 효과를 얻음
- 또한 상대적으로 중요하지 않은 정보(노이즈)를 삭제해 기존의 행렬에서 드러나지 않았던 심층적인 의미 확인

## Latent Semantic Analysis(LSA)
- BoW에 기반한 DTM이나 TF-IDF는 단어의 빈도 수를 이용한 수치화 방법으로 단어의 의미를 고려하지 못함
- LSA는 DTM이나 TF-IDF 행렬에 절단된 SVD를 사용하여 차원을 축소시키고, 단어들의 잠재적인 의미를 끌어냄
- 문서의 유사도 계산 등에서 좋은 성능을 보여줌
- SVD의 특성상 이미 계산된 LSA에 새로운 데이터를 추가하려면 처음부터 다시 계산해야해 업데이트가 어려움

# 19-02. LDA

## Latent Dirichlet Allocation(LDA)
- 문서의 집합으로부터 어떤 토픽이 존재하는지를 알아내기 위한 알고리즘
- 문서들은 토픽들의 혼합으로 구성되어져 있으며, 토픽들은 확률 분포에 기반하여 단어들을 생성한다 가정
- 문서가 작성되었다는 가정 하에 토픽을 뽑아내기 위해 아래 과정을 역으로 추적하는 역공학을 수행

## LDA의 가정
1. 문서에 사용할 단어의 개수 N을 정합
2. 문서에 사용할 토픽의 혼합을 확률 분포에 기반하여 결정
3. 문서에 사용할 각 단어를 (아래와 같이) 정함   
   3-1. 토픽 분포에서 토픽 T를 확률적으로 고름   
   3-2. 선택한 토픽 T에서 단어의 출현 확률 분포에 기반해 문서에 사용할 단어를 고름

## LDA 수행
1. 사용자는 알고리즘에게 토픽의 개수 k를 알려줌
2. 모든 단어를 k개 중 하나의 토픽에 할당
3. 모든 문서의 모든 단어에 대해서 아래의 사항을 반복 진행   
   3-1. 어떤 문서의 단어 w는 자신의 잘못된 토픽에 할당되어 있지만,   
   다른 단어들은 올바른 토픽에 할당되어 있다는 가정 하에 단어 w의 토픽을 재할당

|||
|:-:|:-|
|LSA|DTM을 차원 축소하여 축소 차원에서 근접 단어들을 토픽으로 묶음|
|LDA|단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합확률로 추정하여 토픽 추출|

# 19-08. BERTopic
- BERT embeddings과 클래스 기반 TF-IDF를 활용하여   
  주제 설명에서 중요한 단어를 유지하면서도 쉽게 해석할 수 있는 조밀한 클러스터를 만드는 토픽 모델링 기술

1. 텍스트 데이터를 SBERT로 임베딩
2. 문서를 군집화 (UMAP을 사용해 임베딩의 차원을 줄이고 HDBSCAN 기술을 사용해 클러스터링)
3. 토픽 표현을 생성 (클래스 기반 TF-IDF 토픽 추출)

---

# 20. Text Summarization

## Extractive Summarization
- 원문에서 중요한 핵심 문장 또는 단어구 몇 개를 뽑아서 이들로 구성된 요약문을 만드는 방법
- 추출적 요약의 결과로 나온 요약문의 문장이나 단어구들은 전부 원문에 있는 문장들
- 대표적인 알고리즘으로 머신 러닝 알고리즘 TextRank가 있음

## Abstractive Summarization
- 원문에 없던 문장이라도 핵심 문맥을 반영한 새로운 문장을 생성해서 원문을 요약하는 방법
- 주로 seq2seq 같은 인공 신경망을 이용하지만, 지도 학습이기 때문에 라벨 데이터가 있어야함

## [추상적 요약 구현](https://wikidocs.net/72820)
- [아마존 리뷰 데이터](https://www.kaggle.com/snap/amazon-fine-food-reviews) 사용

## TextRank
- 페이지랭크를 기반으로 한 텍스트 요약 알고리즘으로,   
  텍스트랭크에서 그래프의 노드들은 문장들이며 각 간선의 가중치는 문장들 간의 유사도를 의미
- 사전 훈련된 GloVe 및 [테니스 관련 기사 데이터](https://raw.githubusercontent.com/prateekjoshi565/textrank_text_summarization/master/tennis_articles_v4.csv) 사용

---

# 21. Question Answering(QA)

## Babi 데이터셋
- ID는 각 문장의 번호를 의미, 스토리가 시작될 때는 1번으로 시작
- 라벨의 supporting fact는 실제 정답이 주어진 스토리에서 몇 번 ID 문장에 있었는지를 알려줌

## 메모리 네트워크 구조
- 두 개의 문장, 스토리 문장과 질문 문장이 입력으로 들어오며, 두 문장은 각각 임베딩 과정을 거침
- Embedding C를 통해서 임베딩 된 스토리 문장과 Embedding B를 통해서 임베딩 된 질문 문장은   
  내적을 통해 각 단어 간 유사도를 구하고, 그 결과가 softmax 함수를 지나서   
  Embedding A로 임베딩이 된 스토리 문장에 더해짐   
  (Embedding A, B, C는 각각 별개의 임베딩 층)
- Query(질문 문장)와 Key(스토리 문장)의 유사도를 구하고 softmax 함수를 통해 정규화해   
  Value(스토리 문장)에 더하는 어텐션 메커니즘과 유사
- 어텐션 메커니즘을 통해서 질문 문장과의 유사도를 반영한 스토리 문장 표현을 얻고,   
  이를 질문 표현과 연결(concatenate)하여 LSTM과 밀집층의 입력으로 사용

## [QA 태스크 풀기](https://wikidocs.net/82475)
- [Babi 데이터셋](https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz) 사용

## [MeaN으로 한국어 QA](https://wikidocs.net/85470)
- 한국어 Babi 데이터셋 사용 ([훈련 데이터]((https://bit.ly/31SqtHy)), [테스트 데이터](https://bit.ly/3f7rH5g))

---

# 22. GPT

## Generative Pre-trained Transformer(GPT)
- GPT 설명 [참고](https://huggingface.co/blog/how-to-generate)

## GPT 실습
- [KoGPT-2를 이용한 문장 생성](https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/22.%20Fine-tuning%20GPT-2%20(Cls%2C%20Chatbot%2C%20NLI)/22-2.%20kogpt2_text_generation_gpu.ipynb)
- [KoGPT-2 텍스트 생성을 이용한 한국어 챗봇](https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/22.%20Fine-tuning%20GPT-2%20(Cls%2C%20Chatbot%2C%20NLI)/22-3.%20kogpt2_chatbot_gpu.ipynb)
- [KoGPT-2를 이용한 네이버 영화 리뷰 분류](https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/22.%20Fine-tuning%20GPT-2%20(Cls%2C%20Chatbot%2C%20NLI)/22-4.%20kogpt2_nsmc_tpu.ipynb)
- [KoGPT-2를 이용한 KorNLI 분류](https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/22.%20Fine-tuning%20GPT-2%20(Cls%2C%20Chatbot%2C%20NLI)/22-5.%20kogpt2_kornli_tpu.ipynb)
