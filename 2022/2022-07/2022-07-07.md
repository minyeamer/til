---
layout: post
title: 2022-07-07 Log
date: 2022-07-07 20:00:00 +0900
summary: 꼼꼼한 딥러닝 1
categories: [Study, "2022", "2022-07"]
tags: [TIL, Deep Learning]
cover:
  image: calendar.jpg
---

# Function
- scalar: 양만으로 표시할 수 있는 물리량
- vector: 양과 방향으로 표현할 수 있는 물리량

## 일변수-스칼라함수
- 다항함수, 분수함수, 지수함수, 로그함수, 삼각함수 등
- 입력: 스칼라, 출력: 스칼라
- 독립변수: 함수의 출력 결정, 매개변수: 함수의 모양 결정
- 시그모이드 함수: 모든 실수를 0~1 사이로 찌그러트림, 출력을 확률로 해석

## 다변수-스칼라함수
- 곡면과 등고선 그래프
- 입력: 벡터, 출력: 스칼라
- 평면: 이변수 스칼라 함수, 직선: 우변이 0으로 고정
- 퍼셉트론: $\ f(x,y)=ax+by+c$

## 일변수-벡터함수
- 평면 또는 공간에 존재하는 곡선
- 입력: 스칼라, 출력: 벡터

## 다변수-벡터함수
- 입력: 벡터, 출력: 벡터
- 인공신경망과 유사
- Composite Function: 함수의 합성, $\ g∘f=g(f(x))$
- 신경망은 매우 많은 함수가 겹겹이 합성된 것 (목적함수, 비용함수, 손실함수)
- 머신러닝 분류 문제 평가 함수로 지수,로그함수 활용

---

# Matrix
- 행렬의 차원(크기): 행 개수 x 열 개수 (${m}\times{n}$)
- 행렬의 전치: $\ A^T$
- 행렬의 덧셈: 두 행렬의 크기가 같을 때 같은 위치 요소 덧셈
- 행렬의 곱셈: 행과 열의 요소를 곱해서 더함
- 단위행렬: 대각 성분이 모두 1, 나머지는 0인 정사각 행렬
- 대각행렬: 대각 성분이 아닌 모든 성분이 0인 정사각 행렬
- 대칭행렬: 정사각 행렬에 대해서 $\ S^T=S$
- 직교행렬: 전치된 것이 자신인 역행렬 $\ A^T=A^{-1}$
- 이미지 표현: (C, H, W)=PyTorch 또는 (H, W, C)=TensorFlow
- 열 결합: 뒤에서 곱하는 벡터의 요소를 계수로 한 모든 열의 선형결합
- 행 결합: 앞에서 곱하는 행렬의 행 요소를 계수로 뒷 행렬의 행을 조합 (열 결합 전치)
- 외적: 행렬 반환, 내적: 스칼라 반환

## 인공신경망
- $a^{(1)}=f(W^{(1)}x+b^{(1)})$ (W=weight, b=bias)
- 활성 함수: Sigmoid, ReLU, Tanh
- MNIST 데이터: (28,28)인 손글씨 이미지가 (1,781)인 어레이로 저장
- 로지스틱 함수: 각각의 y에 대한 확률 계산 (다중 분류에서 정확도가 낮음)
- softmax: 실수 k개가 0에서 1사이의 숫자 k개로 맵핑
- 회귀: 입력 x에 대응하는 실수 y값을 출력

---

# Differentiation
- 에러를 줄이 위한 목적 (미분 정보를 이용해 탐색 방향 결정)
- 평균변화율: 함수의 변화를 보는 구간이 중요, 자세한 정보를 위해 간격을 좁힐 필요
- 순간변화율: 평균변화율의 분모를 순간에 이를 정도로 작게 만듦, 극한값이 미분계수
- 순간변화율은 그 위치에서 접선의 기울기
- sympy 패키지를 사용해 파이썬에서 기호 연산

## 미분법
- 상수의 미분, 덧셈/뺄셈의 미분, 곱셈의 미분법, 나눗셈의 미분법
- 함성함수 미분: 라이프니츠 미분법, $\ \sqrt{ax^2+bx+c}=\sqrt{y}$
- $z=\sqrt{x^2+3x},\quad y=x^2+3x,\quad z=\sqrt{y}$
- 연쇄법칙(Chain Rule): $\ \frac{dz}{dx}=\frac{dz}{dy}ㆍ\frac{dy}{dx}$
- 연쇄법칙은 인공신경망(합성함수) 역전파 알고리즘의 기본 아이디어

## 편미분
- 다변수 함수에 구할 수 있는 변화율은 무수히 많음
- 다변수 함수에서 변수 고정, 한번에 변수 하나만 변화 (윤곽선, 1차원 함수)
- 편도함수: 다변수 함수일 때 하나의 변수에 대해서만 미분한 도함수, 기호: $\partial$
- Saliency Map: 이미지 분류에서 y에 대한 다변수-스칼라함수의 편미분 계수를 이미지화
- Guided Back Propagation: saliency map보다 더 확실한 특징을 보임
- 다변수 함수의 연쇄법칙은 모든 노드의 미분계수를 더함

## Jacobian
- 야코비안: $\ w=f_1(x,y),\ z=f_2(x,y)$일 때 편도함수 4개를 블록 형태로 표시
- 로지스틱 함수의 미분: 나눗셈의 미분과 함성함수의 미분을 사용, $\ \frac{d}{dz}\sigma(z)=\sigma(z)(1-\sigma(z))$
- 소프트맥스 함수의 미분: 벡터함수의 편미분 (인데스 별로 나눠서), ${K}\times{K}$개의 미분계수
- 인덱스가 같은 경우 $\ \frac{\partial}{\partial{z_j}}s_i(z)=s_j(z)(1-s_j(z))$
- 인덱스가 다른 경우 $\ \frac{\partial}{\partial{z_j}}s_i(z)=-s_i(z)s_j(z)$

## 직접 미분
- 직접 미분하여 결과를 코딩
- 정확한 결과, 빠른 속도 장점, 미분을 직접 해야하나는 단점

## 수치 미분
- 수치적 계산으로 미분 계수를 근사
- 구현 간단, 변수가 많으면 매우 느리고 수치적으로 불안정
- 자동 미분 계산이 정확한지 확인할 목적으로 사용
- 전방 차분법 diff = (f(x+h) - f(x)) / h
- 중앙 차분법 diff = (f(x+(h/2)) - f(x-(h/2))) / h
- 식을 모를 경우 brute force > 2변수 이상일 때 이동할 수 있는 방향이 무한대가 되는 단점
- gradient descent 방식을 사용해 반복 횟수를 줄임
