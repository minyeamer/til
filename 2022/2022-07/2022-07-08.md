# DL Study
> 꼼꼼한 딥러닝
  - [Optimization](#optimization)
    - [Gradient](#gradient)
    - [Gradient Descent Method](#gradient-descent-method)

---

## Optimization
- minimize(최소화) f(x) subject to(제약조건) h(x)=0
- glaobal minimum, local minimum, local maximum
- 목적함수: 주어진 점과 임의의 선의 간격(error)를 모두 더함

### Gradient
- 경사도 벡터: n개의 변수에 대한 함수 f(x)의 $x^*$에서의 편미분 계수 (열 벡터)
- 경사도 벡터는 $f(x^*)=c$인 표면의 초접평면에 수직, 함수의 최대 증가 방향

### Gradient Descent Method

$$x^{(t+1)}=x^{(t)}-\eta\frac{dy(x^{(t)})}{dx}$$

- minimize 문제를 풀기 위해 gradient 벡터의 반대 방향으로 이동 (최대 하강 방향)
- 강하방향(descent direction): gradient 벡터와의 내적이 0보다 작은 경우 ($c^{(k)}ㆍd^{(k)}<0$)
- $f(x)$ 정식화 가능할 경우 최적성 기준법, 간접법 등, 정식화 할 수 없는 경우 탐색법, 직접법(경사하강법) 등
- 1) 경사도 벡터 계산 2) 강하 방향 선택 3) 이동거리($\alpha$) 결정
- step size($\alpha$)를 결정하기 위해 선탐색(황금분할 탐색) 사용
- 최속 강하법: 경사도 벡터의 반대 반향을 강하 방향으로 선택
- 켤레 경사법: 경사도 벡터의 반대 방향에 이전의 강하 방향을 더함
- 목적 함수로 booth 함수, rosenbrock 함수 사용
