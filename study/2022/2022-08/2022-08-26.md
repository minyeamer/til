---
layout: post
title: 2022-08-26 Log
date: 2022-08-26 20:00:00 +0900
summary: Language models paper reviews
categories: [Study, "2022"]
tags: [TIL, NLP, Paper]
---

# [NNLM](https://youtu.be/bvSHJG-Fz3Y)
- Word Embedding: 의미적으로 유사한 단어를 가까운 벡터 공간에 표현
- one-hot vector: 단어의 개수만큼의 공간을 표현, 모든 단어간 유사도가 0에 가까움
- 어떤 feature vector 표현이 좋고, sequence들의 probability가 높음을 학습
- chain rule에 기반해 이전 단어에 대한 다음 단어의 확률을 계산,   
  단어의 개수가 많아질수록 과거로 봐야할 단어의 수가 많아지기 때문에,   
  Markov assumption을 적용해 n개의 단어만 참조 (n-grams)
- input이 hidden node를 거치지 않고 output으로 연결되는 skip-connection이 존재할 수 있음 (optional)
- NNLM의 목적은 윈도우 내에서 t번째 단어에 대해 t-1번째 단어의 확률이 극대화되는 것을 학습

# [Word2Vec](https://youtu.be/s2KePv-OxZM)
- CBOW: 주변 단어를 가지고 중심 단어를 예측, Skip-gram: 중심 단어를 가지고 주변 단어 예측
- CBOW는 gradient가 각각의 주변 단어에 분산되지만,   
  Skip-gram은 주변 단어의 gradient를 합산하는 방향으로 학습하여 성능이 좋음
- activation function이 존재하지 않는 linear 구조
- Skip-gram의 목적 함수는 t번째 단어에 대해 좌우로 n개의 단어에 대한 확률 합의 최대화
- 대상 단어 벡터와 특정 단어 벡터 간 내적을 하고 총합에 softmax를 취함
- 빈번한 단어는 한쌍으로 묶고, 빈도가 높은 단어의 학습을 제거해 계산을 간소화
- negative sampling: output 단어에 대한 softmax 값을 계산하기 위해 나머지 모든 것들에 대한 내적값을 계산해야 하는데,   
  k 개의 예시에 대해서만 sampling하여 계산의 효율성을 추구

# [FastText](https://youtu.be/7UA21vg4kKE)
- 형태론적 feature 정보를 한 단어의 subwork unit, 문자 단위에서 추출하는 기법
- 기존 방법론은 모든 단어를 각각의 vector로 표현하는 것에 한계가 있고 (OOV 문제),   
  단어의 내부적 구조를 무시하여 유사한 형태의 단어를 표현하지 못함
- skip-gram 기반에 문자 단위 character n-gram을 활용
- 기존 모델의 경우 corpus 대비 context word에 대해서만 학습해 비효율적인 연산이 많은데,   
  negative sampling을 적용해 일정한 확률값으로 뽑인 word를 참고
- 공통된느 형태소들에 대해 parameter sharing을 하여 embedding에 반영 (의미 공유)
- 단어의 시작과 끝 표현 '<', '>'을 추가하고 n개 문자 단위의 벡터를 사용해 계산하며,   
  n-gram이 커지는 문제를 방지하기 위해 hasing function으로 hash값 계산

# [LSTM](https://youtu.be/bX6GLbpw-A4)
- RNN에서 새로운 가중치는 기존 가중치 W - lr * W에 대한 미분값의 chain rule인데,   
  hidden state가 많아질수록 새로운 weight가 기존 weight와 거의 차이가 없어짐
- memory cell을 추가해 새로운 input에 대해 과거의 정보에 대해 몇 퍼센트를 기억할지 결정하고 나머지 정보를 제거
- 일부가 제거된 cell state는 새로운 input에 대한 hidden state와 더해지고,   
  이것이 다시 hidden state와 곱해져 다음 hidden state를 생성

# [Seq2Seq](https://youtu.be/4DzKM0vgG1Y)
- LSTM을 활용한 효율적인 기계 번역 아키텍쳐
- 전통적인 RNN 기반의 기계 번역은 입력과 출력의 크기가 같다고 가정 (입력 토큰과 출력 토큰이 대응)
- 위 문제를 해결하기 위해 인코더에서 고정된 크기의 context vector를 추출,   
  디코더가 문맥 벡터로부터 번역 결과를 추론 (인코더와 디코더는 서로 다른 파라미터를 가짐)
- 시작 시점에 대한 토큰 \<sos>와 종료 시점에 대한 토큰 \<eos>를 추가하여,   
  종료 토큰을 생성할때까지 반복문을 반복
- 입력 문장의 순서를 거꾸로 했을 때 더 높은 정확도를 보임 (앞쪽에 위치한 단어끼리 연관성이 높음)
- Attention을 적용한 Seq2Seq의 경우 모든 hidden states를 디코더로 전달하여,   
  필요한 hidden state를 선택

# [ELMo](https://youtu.be/zV8kIUwH32M)
- ELMo에서의 embedding vector는 bi-directional LSTM에서 가져옴
- forward 및 backward의 각 단계별 hidden state를 concatenate하고,   
  각각의 벡터에 대한 가중합을 통해 embedding을 생성
- 특정 task에 대한 가중합 embedding과 task에 대한 scale vector를 곱해 계산
- forward LM의 경우 k번째 토큰의 j번째 hidden state를 사용하며,   
  마지막 hidden state가 t+1 단어를 예측하는데 사용
- backward LM의 경우 역방향으로 t-1 단어를 예측
- bi-directional LSTM은 forward와 backward에서의 정확도를 함께 최대화
- 2L+1개의 표현 R에 가중합을 취해 하나의 벡터로 만드는데, 태스크에 맞는 j번째 layer에 가중칠르 두고 전반적으로 스케일을 취함
- 가중합을 취하는 방식에 대해선 task별로 각각 취하는 것이 가장 좋고,   
  1/(L+1)로 통합하는 것이 다음, 마지막 hidden state의 가중치를 사용하는 것이 다음으로 좋음
- ELMo의 조합은 downstream task 모델의 input과 output 양쪽에 붙이는게 가장 좋고,   
  input에 붙이는 것, output에 붙이는 것 순으로 좋음

# [GPT](https://youtu.be/o_Wl29aW5XM)
- unlabeled text 데이터가 labeled text 데이터보다 훨씬 많기 때문에,   
  사전학습을 우선 수행하고 labeled 데이터에 대해 fine-tuning을 수행하면 더욱 도움이 될 것
- transformer의 decoder block을 사용하며,   
  encoder-decoder 간의 masked multi-head attention 없이 단순히 쌓아올림
- ELMo는 bi-directional LSTM을 사용하는 반면, GPT는 마스크된 forward를 사용해 다음 단어를 예측
- 일반적인 LM은 전 단계까지의 시퀀스에 대해 i번째 토큰의 likelihood를 최대화하는 것이 목적
- 토큰 임베딩과 포지션 임베딩을 더해 첫번째 hidden state를 만들고,   
  l-1 번째 hidden state를 decoder block을 통과시켜 l번째 hidden state 생성
- GPT는 입력 단어에 대해 정방향으로 예측을 수행하기 때문에 BERT처럼 masking할 필요가 없음
- BERT와 달리 각각의 토큰이 생성되면, 그 다음 토큰을 생성하기 위해 해당 토큰이 사용되는 auto-regressive 방식을 가짐

# [GPT-3](https://youtu.be/xNdp3_Zrr8Q)
- transformer 모델에서 log loss는 scale이 커질수록 개선되고, context도 마찬가지로 scale이 커질수록 정보가 향상됨
- example이 많을수록 성능이 향상되는데 scale이 클수록 차이가 도드라짐
- 기존엔 각각의 example에 대한 결과를 보면서 gradient를 update하면서 fine-tuning 하는데,   
  GPT-3는 zero-shot일 경우 task description과 prompt를 주고,   
  one-shot일 경우 하나의 example, few-shot일 경우 여러 개의 example을 줌 (fine-tuning을 다시 하지 않음)
- fine-tuning은 새로운 데이터셋이 필요하면서, 일반화 성능이 떨어지고 과적합 문제가 발생,   
  few-shot learning은 task-specific 데이터를 사용하지 않지만 SOTA보다는 성능이 떨어짐
- 기존 transformer는 이전 토큰에 대해 모두 attention이 걸리는데,   
  sparse transformer를 사용해 개선
- 학습 데이터와 테스트 데이터 간 overlap이 존재하는데, 비용이 많이 들어 해결하지 못함
- 문서 레벨에서 특정 표현을 반복하는 경우, 매우 긴 문장의 경우 등 특정 task에 대해 성능이 떨어짐
- bidirectional이 아닌 auto-regressive한 구조적 단점
